{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "-------x_text_length:  10000\n",
      "Vocabulary Size: 10539\n",
      "Train/Dev split: 9000/1000\n",
      "WARNING:tensorflow:From C:\\Users\\dkdk6\\cnn-text-classification-tf-master\\text_cnn.py:78: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to C:\\Users\\dkdk6\\cnn-text-classification-tf-master\\runs\\1531944623\n",
      "\n",
      "2018-07-18T13:10:37.367831: step 1, loss 5.10492, acc 0.234375\n",
      "2018-07-18T13:10:49.177247: step 2, loss 5.89032, acc 0.1875\n",
      "2018-07-18T13:11:01.176151: step 3, loss 4.61207, acc 0.234375\n",
      "2018-07-18T13:11:14.683742: step 4, loss 5.07184, acc 0.21875\n",
      "2018-07-18T13:11:26.893075: step 5, loss 3.86303, acc 0.171875\n",
      "2018-07-18T13:11:38.846105: step 6, loss 4.25195, acc 0.25\n",
      "2018-07-18T13:11:50.376266: step 7, loss 3.53748, acc 0.25\n",
      "2018-07-18T13:12:02.585609: step 8, loss 5.39854, acc 0.09375\n",
      "2018-07-18T13:12:13.692907: step 9, loss 4.96005, acc 0.171875\n",
      "2018-07-18T13:12:27.261610: step 10, loss 4.14086, acc 0.21875\n",
      "2018-07-18T13:12:41.076660: step 11, loss 3.61136, acc 0.265625\n",
      "2018-07-18T13:12:56.039639: step 12, loss 3.82045, acc 0.25\n",
      "2018-07-18T13:13:09.075772: step 13, loss 4.15902, acc 0.15625\n",
      "2018-07-18T13:13:21.331990: step 14, loss 4.24744, acc 0.15625\n",
      "2018-07-18T13:13:32.933959: step 15, loss 4.17018, acc 0.21875\n",
      "2018-07-18T13:13:44.486061: step 16, loss 3.49326, acc 0.25\n",
      "2018-07-18T13:13:55.759909: step 17, loss 4.11511, acc 0.28125\n",
      "2018-07-18T13:14:07.687006: step 18, loss 4.0749, acc 0.21875\n",
      "2018-07-18T13:14:18.677610: step 19, loss 4.7017, acc 0.21875\n",
      "2018-07-18T13:14:31.854368: step 20, loss 3.72087, acc 0.234375\n",
      "2018-07-18T13:14:45.567690: step 21, loss 4.57018, acc 0.203125\n",
      "2018-07-18T13:14:58.724501: step 22, loss 3.6521, acc 0.296875\n",
      "2018-07-18T13:15:14.433485: step 23, loss 4.12478, acc 0.234375\n",
      "2018-07-18T13:15:31.042062: step 24, loss 4.04153, acc 0.203125\n",
      "2018-07-18T13:15:45.780641: step 25, loss 4.69298, acc 0.109375\n",
      "2018-07-18T13:16:01.602322: step 26, loss 4.02885, acc 0.203125\n",
      "2018-07-18T13:16:18.412362: step 27, loss 3.28302, acc 0.265625\n",
      "2018-07-18T13:16:32.333129: step 28, loss 3.34109, acc 0.265625\n",
      "2018-07-18T13:16:47.127559: step 29, loss 3.91263, acc 0.203125\n",
      "2018-07-18T13:17:01.778373: step 30, loss 3.50317, acc 0.296875\n",
      "2018-07-18T13:17:14.707791: step 31, loss 3.78861, acc 0.21875\n",
      "2018-07-18T13:17:27.908484: step 32, loss 3.68107, acc 0.234375\n",
      "2018-07-18T13:17:38.976881: step 33, loss 3.49812, acc 0.234375\n",
      "2018-07-18T13:17:52.290047: step 34, loss 3.6877, acc 0.25\n",
      "2018-07-18T13:18:06.129032: step 35, loss 2.91023, acc 0.328125\n",
      "2018-07-18T13:18:19.211041: step 36, loss 3.39123, acc 0.171875\n",
      "2018-07-18T13:18:31.748509: step 37, loss 3.30111, acc 0.25\n",
      "2018-07-18T13:18:42.214515: step 38, loss 3.34518, acc 0.25\n",
      "2018-07-18T13:18:51.907591: step 39, loss 3.38264, acc 0.296875\n",
      "2018-07-18T13:19:02.271868: step 40, loss 3.08238, acc 0.234375\n",
      "2018-07-18T13:19:15.340915: step 41, loss 3.73225, acc 0.25\n",
      "2018-07-18T13:19:24.440576: step 42, loss 4.21795, acc 0.25\n",
      "2018-07-18T13:19:33.860381: step 43, loss 3.24979, acc 0.21875\n",
      "2018-07-18T13:19:42.593024: step 44, loss 3.62336, acc 0.15625\n",
      "2018-07-18T13:19:51.537102: step 45, loss 3.34591, acc 0.265625\n",
      "2018-07-18T13:20:01.329910: step 46, loss 3.23852, acc 0.25\n",
      "2018-07-18T13:20:12.563863: step 47, loss 3.2206, acc 0.25\n",
      "2018-07-18T13:20:26.849654: step 48, loss 3.43088, acc 0.234375\n",
      "2018-07-18T13:20:40.273750: step 49, loss 2.6187, acc 0.296875\n",
      "2018-07-18T13:20:49.960839: step 50, loss 3.61053, acc 0.25\n",
      "2018-07-18T13:20:59.417546: step 51, loss 3.96504, acc 0.1875\n",
      "2018-07-18T13:21:11.507210: step 52, loss 2.98636, acc 0.265625\n",
      "2018-07-18T13:21:22.008123: step 53, loss 2.66465, acc 0.1875\n",
      "2018-07-18T13:21:32.812227: step 54, loss 3.82774, acc 0.203125\n",
      "2018-07-18T13:21:44.555817: step 55, loss 2.8585, acc 0.1875\n",
      "2018-07-18T13:21:58.482568: step 56, loss 3.3278, acc 0.265625\n",
      "2018-07-18T13:22:10.458535: step 57, loss 3.22494, acc 0.265625\n",
      "2018-07-18T13:22:20.747018: step 58, loss 3.53844, acc 0.15625\n",
      "2018-07-18T13:22:32.672123: step 59, loss 3.72669, acc 0.1875\n",
      "2018-07-18T13:22:42.883809: step 60, loss 2.87862, acc 0.234375\n",
      "2018-07-18T13:22:53.375747: step 61, loss 2.95012, acc 0.25\n",
      "2018-07-18T13:23:04.006314: step 62, loss 3.49759, acc 0.265625\n",
      "2018-07-18T13:23:14.130237: step 63, loss 3.91493, acc 0.234375\n",
      "2018-07-18T13:23:24.253160: step 64, loss 3.17122, acc 0.234375\n",
      "2018-07-18T13:23:34.518703: step 65, loss 3.35183, acc 0.296875\n",
      "2018-07-18T13:23:45.015628: step 66, loss 3.63866, acc 0.234375\n",
      "2018-07-18T13:23:55.026852: step 67, loss 2.73917, acc 0.28125\n",
      "2018-07-18T13:24:04.979232: step 68, loss 3.11239, acc 0.234375\n",
      "2018-07-18T13:24:15.077223: step 69, loss 2.84013, acc 0.34375\n",
      "2018-07-18T13:24:28.781570: step 70, loss 3.18681, acc 0.34375\n",
      "2018-07-18T13:24:43.253425: step 71, loss 3.12933, acc 0.21875\n",
      "2018-07-18T13:24:57.715746: step 72, loss 3.73241, acc 0.3125\n",
      "2018-07-18T13:25:09.239920: step 73, loss 3.13114, acc 0.265625\n",
      "2018-07-18T13:25:19.504466: step 74, loss 3.57078, acc 0.28125\n",
      "2018-07-18T13:25:31.773651: step 75, loss 3.50934, acc 0.203125\n",
      "2018-07-18T13:25:43.243973: step 76, loss 2.7592, acc 0.3125\n",
      "2018-07-18T13:25:53.916426: step 77, loss 3.29514, acc 0.15625\n",
      "2018-07-18T13:26:04.223858: step 78, loss 2.55725, acc 0.40625\n",
      "2018-07-18T13:26:15.995372: step 79, loss 3.0737, acc 0.203125\n",
      "2018-07-18T13:26:27.344019: step 80, loss 3.29449, acc 0.234375\n",
      "2018-07-18T13:26:38.158103: step 81, loss 2.84329, acc 0.40625\n",
      "2018-07-18T13:26:49.448798: step 82, loss 3.41564, acc 0.25\n",
      "2018-07-18T13:27:00.518191: step 83, loss 2.82001, acc 0.359375\n",
      "2018-07-18T13:27:11.452944: step 84, loss 3.17793, acc 0.296875\n",
      "2018-07-18T13:27:22.074535: step 85, loss 2.91002, acc 0.296875\n",
      "2018-07-18T13:27:33.955757: step 86, loss 2.73939, acc 0.375\n",
      "2018-07-18T13:27:46.190035: step 87, loss 2.51055, acc 0.34375\n",
      "2018-07-18T13:27:58.082229: step 88, loss 2.56981, acc 0.390625\n",
      "2018-07-18T13:28:08.961131: step 89, loss 2.96521, acc 0.28125\n",
      "2018-07-18T13:28:20.403525: step 90, loss 2.96954, acc 0.265625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-18T13:28:31.787078: step 91, loss 3.10176, acc 0.34375\n",
      "2018-07-18T13:28:43.756066: step 92, loss 3.13486, acc 0.21875\n",
      "2018-07-18T13:28:54.592083: step 93, loss 2.89378, acc 0.15625\n",
      "2018-07-18T13:29:05.363274: step 94, loss 2.54536, acc 0.34375\n",
      "2018-07-18T13:29:15.204950: step 95, loss 2.73049, acc 0.3125\n",
      "2018-07-18T13:29:25.025683: step 96, loss 2.37664, acc 0.375\n",
      "2018-07-18T13:29:35.103728: step 97, loss 3.02841, acc 0.328125\n",
      "2018-07-18T13:29:45.051123: step 98, loss 2.85416, acc 0.3125\n",
      "2018-07-18T13:29:55.241865: step 99, loss 2.22144, acc 0.375\n",
      "2018-07-18T13:30:04.690594: step 100, loss 2.72093, acc 0.296875\n",
      "\n",
      "Evaluation:\n",
      "2018-07-18T13:31:53.811655: step 100, loss 1.40876, acc 0.421\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\dkdk6\\cnn-text-classification-tf-master\\runs\\1531944623\\checkpoints\\model-100\n",
      "\n",
      "2018-07-18T13:32:05.209172: step 101, loss 2.93039, acc 0.359375\n",
      "2018-07-18T13:32:15.925508: step 102, loss 2.27612, acc 0.390625\n",
      "2018-07-18T13:32:26.538123: step 103, loss 3.58479, acc 0.296875\n",
      "2018-07-18T13:32:36.296025: step 104, loss 2.69307, acc 0.21875\n",
      "2018-07-18T13:32:46.281319: step 105, loss 2.87706, acc 0.28125\n",
      "2018-07-18T13:32:56.633628: step 106, loss 2.82015, acc 0.296875\n",
      "2018-07-18T13:33:08.144841: step 107, loss 2.93525, acc 0.3125\n",
      "2018-07-18T13:33:19.776728: step 108, loss 2.64613, acc 0.328125\n",
      "2018-07-18T13:33:31.277345: step 109, loss 2.6731, acc 0.375\n",
      "2018-07-18T13:33:42.884300: step 110, loss 3.01217, acc 0.25\n",
      "2018-07-18T13:33:55.029190: step 111, loss 2.67393, acc 0.3125\n",
      "2018-07-18T13:34:07.814993: step 112, loss 2.99049, acc 0.3125\n",
      "2018-07-18T13:34:19.000077: step 113, loss 2.57681, acc 0.34375\n",
      "2018-07-18T13:34:30.862349: step 114, loss 2.88196, acc 0.265625\n",
      "2018-07-18T13:34:41.716319: step 115, loss 2.92712, acc 0.203125\n",
      "2018-07-18T13:34:52.967227: step 116, loss 2.34289, acc 0.34375\n",
      "2018-07-18T13:35:05.207487: step 117, loss 2.65096, acc 0.28125\n",
      "2018-07-18T13:35:16.853811: step 118, loss 2.73308, acc 0.3125\n",
      "2018-07-18T13:35:27.152265: step 119, loss 2.53054, acc 0.421875\n",
      "2018-07-18T13:35:37.823723: step 120, loss 3.08438, acc 0.28125\n",
      "2018-07-18T13:35:47.505828: step 121, loss 2.3865, acc 0.390625\n",
      "2018-07-18T13:35:56.993451: step 122, loss 3.04639, acc 0.234375\n",
      "2018-07-18T13:36:06.248696: step 123, loss 2.6875, acc 0.421875\n",
      "2018-07-18T13:36:15.705402: step 124, loss 3.36388, acc 0.25\n",
      "2018-07-18T13:36:24.998546: step 125, loss 2.88893, acc 0.296875\n",
      "2018-07-18T13:36:35.739818: step 126, loss 2.52834, acc 0.28125\n",
      "2018-07-18T13:36:46.870047: step 127, loss 2.8775, acc 0.3125\n",
      "2018-07-18T13:36:58.319426: step 128, loss 2.91707, acc 0.25\n",
      "2018-07-18T13:37:08.574996: step 129, loss 1.78335, acc 0.421875\n",
      "2018-07-18T13:37:19.572581: step 130, loss 2.82772, acc 0.3125\n",
      "2018-07-18T13:37:30.832464: step 131, loss 2.72096, acc 0.234375\n",
      "2018-07-18T13:37:42.663819: step 132, loss 2.15796, acc 0.359375\n",
      "2018-07-18T13:37:53.527763: step 133, loss 2.81004, acc 0.25\n",
      "2018-07-18T13:38:05.823874: step 134, loss 2.15184, acc 0.375\n",
      "2018-07-18T13:38:17.679165: step 135, loss 2.00707, acc 0.375\n",
      "2018-07-18T13:38:28.886190: step 136, loss 2.95992, acc 0.28125\n",
      "2018-07-18T13:38:39.911701: step 137, loss 2.54315, acc 0.3125\n",
      "2018-07-18T13:38:51.422492: step 138, loss 2.41103, acc 0.40625\n",
      "2018-07-18T13:39:02.268483: step 139, loss 2.58945, acc 0.359375\n",
      "2018-07-18T13:39:12.536020: step 140, loss 2.58231, acc 0.34375\n",
      "2018-07-18T13:39:19.310900: step 141, loss 2.49745, acc 0.375\n",
      "2018-07-18T13:39:28.830439: step 142, loss 2.60357, acc 0.265625\n",
      "2018-07-18T13:39:38.715000: step 143, loss 2.4189, acc 0.359375\n",
      "2018-07-18T13:39:47.755820: step 144, loss 1.85076, acc 0.4375\n",
      "2018-07-18T13:39:58.459847: step 145, loss 1.96284, acc 0.328125\n",
      "2018-07-18T13:40:09.388617: step 146, loss 2.43584, acc 0.34375\n",
      "2018-07-18T13:40:20.220646: step 147, loss 2.43695, acc 0.34375\n",
      "2018-07-18T13:40:31.474545: step 148, loss 1.92081, acc 0.390625\n",
      "2018-07-18T13:40:42.300589: step 149, loss 2.92887, acc 0.3125\n",
      "2018-07-18T13:40:54.683468: step 150, loss 2.07055, acc 0.328125\n",
      "2018-07-18T13:41:04.752538: step 151, loss 1.93472, acc 0.40625\n",
      "2018-07-18T13:41:16.801312: step 152, loss 1.93866, acc 0.4375\n",
      "2018-07-18T13:41:29.734719: step 153, loss 1.77906, acc 0.359375\n",
      "2018-07-18T13:41:40.001260: step 154, loss 2.27077, acc 0.390625\n",
      "2018-07-18T13:41:53.436326: step 155, loss 2.38204, acc 0.40625\n",
      "2018-07-18T13:42:04.035975: step 156, loss 2.33872, acc 0.46875\n",
      "2018-07-18T13:42:14.260627: step 157, loss 2.16768, acc 0.421875\n",
      "2018-07-18T13:42:25.814724: step 158, loss 2.17147, acc 0.296875\n",
      "2018-07-18T13:42:37.091562: step 159, loss 1.69105, acc 0.421875\n",
      "2018-07-18T13:42:48.760352: step 160, loss 2.39822, acc 0.40625\n",
      "2018-07-18T13:42:59.899560: step 161, loss 1.91769, acc 0.53125\n",
      "2018-07-18T13:43:10.979922: step 162, loss 2.1683, acc 0.359375\n",
      "2018-07-18T13:43:22.524047: step 163, loss 2.14984, acc 0.375\n",
      "2018-07-18T13:43:33.355077: step 164, loss 2.48309, acc 0.359375\n",
      "2018-07-18T13:43:45.872597: step 165, loss 1.90306, acc 0.453125\n",
      "2018-07-18T13:43:57.594246: step 166, loss 2.21822, acc 0.421875\n",
      "2018-07-18T13:44:10.431910: step 167, loss 2.1081, acc 0.375\n",
      "2018-07-18T13:44:23.031211: step 168, loss 2.38931, acc 0.5\n",
      "2018-07-18T13:44:34.084647: step 169, loss 2.16811, acc 0.296875\n",
      "2018-07-18T13:44:45.081236: step 170, loss 2.4903, acc 0.34375\n",
      "2018-07-18T13:44:58.036584: step 171, loss 1.89769, acc 0.453125\n",
      "2018-07-18T13:45:10.063417: step 172, loss 2.06243, acc 0.4375\n",
      "2018-07-18T13:45:20.684010: step 173, loss 2.06026, acc 0.421875\n",
      "2018-07-18T13:45:32.768688: step 174, loss 2.02329, acc 0.453125\n",
      "2018-07-18T13:45:45.456763: step 175, loss 1.73956, acc 0.453125\n",
      "2018-07-18T13:45:56.482262: step 176, loss 1.9689, acc 0.453125\n",
      "2018-07-18T13:46:07.054985: step 177, loss 2.1664, acc 0.296875\n",
      "2018-07-18T13:46:16.914612: step 178, loss 1.91387, acc 0.390625\n",
      "2018-07-18T13:46:26.410216: step 179, loss 2.00168, acc 0.4375\n",
      "2018-07-18T13:46:35.864927: step 180, loss 2.12167, acc 0.40625\n",
      "2018-07-18T13:46:45.295705: step 181, loss 2.45915, acc 0.359375\n",
      "2018-07-18T13:46:55.477470: step 182, loss 2.29792, acc 0.34375\n",
      "2018-07-18T13:47:06.173862: step 183, loss 1.86796, acc 0.390625\n",
      "2018-07-18T13:47:15.557761: step 184, loss 1.72513, acc 0.4375\n",
      "2018-07-18T13:47:24.814006: step 185, loss 1.85019, acc 0.4375\n",
      "2018-07-18T13:47:34.037340: step 186, loss 2.3287, acc 0.328125\n",
      "2018-07-18T13:47:43.363392: step 187, loss 1.83627, acc 0.375\n",
      "2018-07-18T13:47:52.639581: step 188, loss 2.11638, acc 0.484375\n",
      "2018-07-18T13:48:01.976608: step 189, loss 1.64858, acc 0.578125\n",
      "2018-07-18T13:48:12.553318: step 190, loss 1.82001, acc 0.453125\n",
      "2018-07-18T13:48:23.727431: step 191, loss 1.76307, acc 0.34375\n",
      "2018-07-18T13:48:36.034515: step 192, loss 1.99504, acc 0.421875\n",
      "2018-07-18T13:48:47.890804: step 193, loss 1.94353, acc 0.4375\n",
      "2018-07-18T13:48:59.431936: step 194, loss 2.53926, acc 0.328125\n",
      "2018-07-18T13:49:12.546858: step 195, loss 1.75925, acc 0.46875\n",
      "2018-07-18T13:49:24.675418: step 196, loss 2.02531, acc 0.40625\n",
      "2018-07-18T13:49:37.485156: step 197, loss 1.86279, acc 0.453125\n",
      "2018-07-18T13:49:47.037606: step 198, loss 1.92933, acc 0.40625\n",
      "2018-07-18T13:49:59.663836: step 199, loss 2.07046, acc 0.375\n",
      "2018-07-18T13:50:12.094589: step 200, loss 2.04936, acc 0.4375\n",
      "\n",
      "Evaluation:\n",
      "2018-07-18T13:52:03.298156: step 200, loss 1.20157, acc 0.489\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\dkdk6\\cnn-text-classification-tf-master\\runs\\1531944623\\checkpoints\\model-200\n",
      "\n",
      "2018-07-18T13:52:15.345933: step 201, loss 2.08288, acc 0.34375\n",
      "2018-07-18T13:52:24.968197: step 202, loss 2.03547, acc 0.453125\n",
      "2018-07-18T13:52:34.688200: step 203, loss 1.96623, acc 0.5\n",
      "2018-07-18T13:52:44.185796: step 204, loss 1.77724, acc 0.453125\n",
      "2018-07-18T13:52:53.752211: step 205, loss 2.12486, acc 0.328125\n",
      "2018-07-18T13:53:03.214901: step 206, loss 2.18547, acc 0.34375\n",
      "2018-07-18T13:53:12.749398: step 207, loss 1.76183, acc 0.4375\n",
      "2018-07-18T13:53:22.241015: step 208, loss 1.69119, acc 0.40625\n",
      "2018-07-18T13:53:31.745591: step 209, loss 2.01508, acc 0.375\n",
      "2018-07-18T13:53:41.289064: step 210, loss 2.09385, acc 0.328125\n",
      "2018-07-18T13:53:50.807607: step 211, loss 1.94438, acc 0.4375\n",
      "2018-07-18T13:54:00.331134: step 212, loss 1.94306, acc 0.4375\n",
      "2018-07-18T13:54:11.329717: step 213, loss 2.33558, acc 0.28125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-18T13:54:21.503506: step 214, loss 1.91329, acc 0.4375\n",
      "2018-07-18T13:54:31.679289: step 215, loss 2.1654, acc 0.359375\n",
      "2018-07-18T13:54:42.876341: step 216, loss 1.69333, acc 0.5\n",
      "2018-07-18T13:54:53.492946: step 217, loss 2.4624, acc 0.3125\n",
      "2018-07-18T13:55:03.336616: step 218, loss 1.95449, acc 0.421875\n",
      "2018-07-18T13:55:14.134735: step 219, loss 2.28733, acc 0.359375\n",
      "2018-07-18T13:55:24.476076: step 220, loss 1.90173, acc 0.28125\n",
      "2018-07-18T13:55:34.114297: step 221, loss 1.58013, acc 0.453125\n",
      "2018-07-18T13:55:44.046731: step 222, loss 1.97734, acc 0.421875\n",
      "2018-07-18T13:55:54.269389: step 223, loss 1.98063, acc 0.34375\n",
      "2018-07-18T13:56:04.473098: step 224, loss 2.15563, acc 0.375\n",
      "2018-07-18T13:56:14.202076: step 225, loss 1.70875, acc 0.4375\n",
      "2018-07-18T13:56:24.095615: step 226, loss 1.6982, acc 0.40625\n",
      "2018-07-18T13:56:35.412360: step 227, loss 1.81428, acc 0.40625\n",
      "2018-07-18T13:56:46.277286: step 228, loss 2.03463, acc 0.359375\n",
      "2018-07-18T13:56:57.040498: step 229, loss 1.99371, acc 0.40625\n",
      "2018-07-18T13:57:06.712628: step 230, loss 1.77041, acc 0.421875\n",
      "2018-07-18T13:57:18.608810: step 231, loss 1.96892, acc 0.3125\n",
      "2018-07-18T13:57:29.370027: step 232, loss 1.73175, acc 0.46875\n",
      "2018-07-18T13:57:39.429124: step 233, loss 1.59716, acc 0.5\n",
      "2018-07-18T13:57:49.886155: step 234, loss 2.25471, acc 0.359375\n",
      "2018-07-18T13:58:00.251432: step 235, loss 1.47922, acc 0.515625\n",
      "2018-07-18T13:58:10.461123: step 236, loss 2.0523, acc 0.375\n",
      "2018-07-18T13:58:20.420487: step 237, loss 1.82347, acc 0.359375\n",
      "2018-07-18T13:58:30.925389: step 238, loss 1.77148, acc 0.421875\n",
      "2018-07-18T13:58:40.877770: step 239, loss 2.23812, acc 0.3125\n",
      "2018-07-18T13:58:53.668559: step 240, loss 1.82531, acc 0.453125\n",
      "2018-07-18T13:59:06.709680: step 241, loss 1.94545, acc 0.421875\n",
      "2018-07-18T13:59:19.705918: step 242, loss 1.9003, acc 0.46875\n",
      "2018-07-18T13:59:30.335487: step 243, loss 1.84648, acc 0.4375\n",
      "2018-07-18T13:59:42.058133: step 244, loss 1.42026, acc 0.578125\n",
      "2018-07-18T13:59:52.784444: step 245, loss 1.84264, acc 0.4375\n",
      "2018-07-18T14:00:03.983490: step 246, loss 2.2729, acc 0.375\n",
      "2018-07-18T14:00:15.876680: step 247, loss 1.65016, acc 0.453125\n",
      "2018-07-18T14:00:27.636228: step 248, loss 1.65268, acc 0.390625\n",
      "2018-07-18T14:00:39.358876: step 249, loss 2.05776, acc 0.4375\n",
      "2018-07-18T14:00:51.835505: step 250, loss 1.65823, acc 0.375\n",
      "2018-07-18T14:01:03.955087: step 251, loss 2.13745, acc 0.390625\n",
      "2018-07-18T14:01:15.159121: step 252, loss 1.54514, acc 0.453125\n",
      "2018-07-18T14:01:27.244796: step 253, loss 1.79164, acc 0.40625\n",
      "2018-07-18T14:01:37.821506: step 254, loss 2.06414, acc 0.4375\n",
      "2018-07-18T14:01:49.478328: step 255, loss 1.95819, acc 0.375\n",
      "2018-07-18T14:02:00.782096: step 256, loss 1.92432, acc 0.421875\n",
      "2018-07-18T14:02:12.104811: step 257, loss 1.85994, acc 0.359375\n",
      "2018-07-18T14:02:24.356043: step 258, loss 2.04763, acc 0.390625\n",
      "2018-07-18T14:02:37.341312: step 259, loss 1.3534, acc 0.515625\n",
      "2018-07-18T14:02:49.644406: step 260, loss 1.77477, acc 0.4375\n",
      "2018-07-18T14:03:02.968768: step 261, loss 1.82167, acc 0.40625\n",
      "2018-07-18T14:03:13.504588: step 262, loss 2.39881, acc 0.34375\n",
      "2018-07-18T14:03:23.342274: step 263, loss 2.14631, acc 0.40625\n",
      "2018-07-18T14:03:33.459215: step 264, loss 1.55987, acc 0.453125\n",
      "2018-07-18T14:03:45.905925: step 265, loss 1.81051, acc 0.359375\n",
      "2018-07-18T14:03:57.669462: step 266, loss 1.74308, acc 0.546875\n",
      "2018-07-18T14:04:08.438658: step 267, loss 1.84844, acc 0.421875\n",
      "2018-07-18T14:04:20.592152: step 268, loss 1.74235, acc 0.515625\n",
      "2018-07-18T14:04:31.994655: step 269, loss 1.87781, acc 0.40625\n",
      "2018-07-18T14:04:41.631876: step 270, loss 1.5452, acc 0.484375\n",
      "2018-07-18T14:04:51.132467: step 271, loss 1.99964, acc 0.40625\n",
      "2018-07-18T14:05:01.850800: step 272, loss 1.92789, acc 0.484375\n",
      "2018-07-18T14:05:11.458102: step 273, loss 1.67575, acc 0.453125\n",
      "2018-07-18T14:05:20.975651: step 274, loss 2.16914, acc 0.359375\n",
      "2018-07-18T14:05:30.679691: step 275, loss 1.69137, acc 0.5\n",
      "2018-07-18T14:05:40.281011: step 276, loss 1.59285, acc 0.421875\n",
      "2018-07-18T14:05:49.936187: step 277, loss 2.12706, acc 0.328125\n",
      "2018-07-18T14:05:59.732983: step 278, loss 2.07507, acc 0.3125\n",
      "2018-07-18T14:06:09.243547: step 279, loss 1.75523, acc 0.359375\n",
      "2018-07-18T14:06:18.706238: step 280, loss 1.59946, acc 0.484375\n",
      "2018-07-18T14:06:28.110085: step 281, loss 1.63902, acc 0.421875\n",
      "2018-07-18T14:06:33.948470: step 282, loss 1.46703, acc 0.425\n",
      "2018-07-18T14:06:43.422132: step 283, loss 1.42633, acc 0.515625\n",
      "2018-07-18T14:06:54.087604: step 284, loss 2.05146, acc 0.4375\n",
      "2018-07-18T14:07:04.978476: step 285, loss 1.28134, acc 0.546875\n",
      "2018-07-18T14:07:21.031539: step 286, loss 1.89073, acc 0.28125\n",
      "2018-07-18T14:07:33.080312: step 287, loss 1.74205, acc 0.4375\n",
      "2018-07-18T14:07:44.682283: step 288, loss 1.67086, acc 0.453125\n",
      "2018-07-18T14:07:54.974753: step 289, loss 1.70418, acc 0.46875\n",
      "2018-07-18T14:08:07.131936: step 290, loss 1.68491, acc 0.484375\n",
      "2018-07-18T14:08:16.895820: step 291, loss 1.49608, acc 0.359375\n",
      "2018-07-18T14:08:27.563288: step 292, loss 1.34485, acc 0.5625\n",
      "2018-07-18T14:08:39.938188: step 293, loss 1.53094, acc 0.53125\n",
      "2018-07-18T14:08:49.976340: step 294, loss 1.4495, acc 0.46875\n",
      "2018-07-18T14:08:59.691357: step 295, loss 1.26741, acc 0.484375\n",
      "2018-07-18T14:09:09.056309: step 296, loss 1.76234, acc 0.40625\n",
      "2018-07-18T14:09:18.540940: step 297, loss 1.49774, acc 0.5\n",
      "2018-07-18T14:09:28.379625: step 298, loss 1.40594, acc 0.53125\n",
      "2018-07-18T14:09:37.741584: step 299, loss 1.44109, acc 0.46875\n",
      "2018-07-18T14:09:46.981870: step 300, loss 1.52687, acc 0.390625\n",
      "\n",
      "Evaluation:\n",
      "2018-07-18T14:11:20.406990: step 300, loss 1.08952, acc 0.572\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\dkdk6\\cnn-text-classification-tf-master\\runs\\1531944623\\checkpoints\\model-300\n",
      "\n",
      "2018-07-18T14:11:29.602396: step 301, loss 2.04223, acc 0.40625\n",
      "2018-07-18T14:11:38.046811: step 302, loss 1.65176, acc 0.46875\n",
      "2018-07-18T14:11:46.553059: step 303, loss 1.36008, acc 0.53125\n",
      "2018-07-18T14:11:54.897740: step 304, loss 1.47127, acc 0.4375\n",
      "2018-07-18T14:12:03.310240: step 305, loss 1.42841, acc 0.46875\n",
      "2018-07-18T14:12:11.703790: step 306, loss 1.32856, acc 0.5\n",
      "2018-07-18T14:12:21.205376: step 307, loss 1.49243, acc 0.359375\n",
      "2018-07-18T14:12:31.723245: step 308, loss 1.72403, acc 0.421875\n",
      "2018-07-18T14:12:40.517723: step 309, loss 1.51495, acc 0.5\n",
      "2018-07-18T14:12:53.140960: step 310, loss 1.55458, acc 0.46875\n",
      "2018-07-18T14:13:04.350977: step 311, loss 1.79517, acc 0.4375\n",
      "2018-07-18T14:13:13.893456: step 312, loss 1.50289, acc 0.515625\n",
      "2018-07-18T14:13:23.869771: step 313, loss 1.32484, acc 0.40625\n",
      "2018-07-18T14:13:33.379336: step 314, loss 1.62613, acc 0.453125\n",
      "2018-07-18T14:13:43.486304: step 315, loss 1.16448, acc 0.53125\n",
      "2018-07-18T14:13:53.127517: step 316, loss 1.41598, acc 0.5\n",
      "2018-07-18T14:14:03.207556: step 317, loss 1.46954, acc 0.46875\n",
      "2018-07-18T14:14:13.455148: step 318, loss 1.77596, acc 0.40625\n",
      "2018-07-18T14:14:22.920830: step 319, loss 1.39846, acc 0.515625\n",
      "2018-07-18T14:14:33.128528: step 320, loss 1.65032, acc 0.4375\n",
      "2018-07-18T14:14:41.403396: step 321, loss 1.36837, acc 0.578125\n",
      "2018-07-18T14:14:52.617134: step 322, loss 1.39494, acc 0.5625\n",
      "2018-07-18T14:15:02.649302: step 323, loss 2.01667, acc 0.34375\n",
      "2018-07-18T14:15:13.624945: step 324, loss 1.60579, acc 0.4375\n",
      "2018-07-18T14:15:23.249203: step 325, loss 1.05923, acc 0.625\n",
      "2018-07-18T14:15:32.756774: step 326, loss 1.4747, acc 0.484375\n",
      "2018-07-18T14:15:41.584165: step 327, loss 1.30215, acc 0.46875\n",
      "2018-07-18T14:15:50.606033: step 328, loss 1.47308, acc 0.453125\n",
      "2018-07-18T14:15:59.915135: step 329, loss 1.56133, acc 0.484375\n",
      "2018-07-18T14:16:09.223239: step 330, loss 1.48236, acc 0.46875\n",
      "2018-07-18T14:16:18.260069: step 331, loss 1.15637, acc 0.59375\n",
      "2018-07-18T14:16:27.271965: step 332, loss 1.64968, acc 0.40625\n",
      "2018-07-18T14:16:36.345696: step 333, loss 1.56287, acc 0.421875\n",
      "2018-07-18T14:16:45.488244: step 334, loss 1.45261, acc 0.484375\n",
      "2018-07-18T14:16:54.834245: step 335, loss 1.58449, acc 0.453125\n",
      "2018-07-18T14:17:04.686894: step 336, loss 1.36323, acc 0.59375\n",
      "2018-07-18T14:17:14.484688: step 337, loss 1.64628, acc 0.5\n",
      "2018-07-18T14:17:25.272833: step 338, loss 1.34883, acc 0.5\n",
      "2018-07-18T14:17:35.193299: step 339, loss 1.61837, acc 0.453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-18T14:17:45.023008: step 340, loss 1.65063, acc 0.4375\n",
      "2018-07-18T14:17:55.295532: step 341, loss 1.28523, acc 0.515625\n",
      "2018-07-18T14:18:05.531157: step 342, loss 1.21802, acc 0.5625\n",
      "2018-07-18T14:18:14.226899: step 343, loss 1.54785, acc 0.4375\n",
      "2018-07-18T14:18:22.810938: step 344, loss 1.69342, acc 0.390625\n",
      "2018-07-18T14:18:31.385006: step 345, loss 1.31445, acc 0.5625\n",
      "2018-07-18T14:18:39.846375: step 346, loss 1.42363, acc 0.4375\n",
      "2018-07-18T14:18:48.323700: step 347, loss 1.41059, acc 0.46875\n",
      "2018-07-18T14:18:56.732211: step 348, loss 1.41576, acc 0.546875\n",
      "2018-07-18T14:19:05.383073: step 349, loss 1.41621, acc 0.453125\n",
      "2018-07-18T14:19:13.902287: step 350, loss 1.58809, acc 0.40625\n",
      "2018-07-18T14:19:22.443443: step 351, loss 1.4225, acc 0.453125\n",
      "2018-07-18T14:19:31.069371: step 352, loss 1.49878, acc 0.5\n",
      "2018-07-18T14:19:39.705273: step 353, loss 1.27601, acc 0.546875\n",
      "2018-07-18T14:19:48.385058: step 354, loss 1.35405, acc 0.609375\n",
      "2018-07-18T14:19:57.416900: step 355, loss 1.31006, acc 0.578125\n",
      "2018-07-18T14:20:06.190434: step 356, loss 1.26435, acc 0.546875\n",
      "2018-07-18T14:20:15.044753: step 357, loss 2.001, acc 0.390625\n",
      "2018-07-18T14:20:23.815294: step 358, loss 1.8557, acc 0.40625\n",
      "2018-07-18T14:20:32.906977: step 359, loss 1.31307, acc 0.546875\n",
      "2018-07-18T14:20:42.621993: step 360, loss 1.27332, acc 0.546875\n",
      "2018-07-18T14:20:51.416470: step 361, loss 1.17467, acc 0.578125\n",
      "2018-07-18T14:21:00.213940: step 362, loss 1.80102, acc 0.4375\n",
      "2018-07-18T14:21:08.919655: step 363, loss 1.53956, acc 0.4375\n",
      "2018-07-18T14:21:17.763002: step 364, loss 1.62022, acc 0.40625\n",
      "2018-07-18T14:21:26.509609: step 365, loss 1.32395, acc 0.46875\n",
      "2018-07-18T14:21:35.184407: step 366, loss 1.36217, acc 0.515625\n",
      "2018-07-18T14:21:43.963925: step 367, loss 1.35504, acc 0.484375\n",
      "2018-07-18T14:21:52.821234: step 368, loss 1.47274, acc 0.390625\n",
      "2018-07-18T14:22:01.565845: step 369, loss 1.62585, acc 0.4375\n",
      "2018-07-18T14:22:10.682462: step 370, loss 1.53561, acc 0.515625\n",
      "2018-07-18T14:22:20.079328: step 371, loss 1.54365, acc 0.453125\n",
      "2018-07-18T14:22:28.795016: step 372, loss 1.13507, acc 0.515625\n",
      "2018-07-18T14:22:37.750066: step 373, loss 1.93468, acc 0.390625\n",
      "2018-07-18T14:22:46.300196: step 374, loss 1.575, acc 0.453125\n",
      "2018-07-18T14:22:54.742616: step 375, loss 1.14231, acc 0.5625\n",
      "2018-07-18T14:23:03.526130: step 376, loss 1.24846, acc 0.484375\n",
      "2018-07-18T14:23:13.898381: step 377, loss 1.28818, acc 0.515625\n",
      "2018-07-18T14:23:22.268993: step 378, loss 1.2733, acc 0.46875\n",
      "2018-07-18T14:23:30.870986: step 379, loss 1.66522, acc 0.390625\n",
      "2018-07-18T14:23:39.331356: step 380, loss 1.36424, acc 0.46875\n",
      "2018-07-18T14:23:49.182010: step 381, loss 1.41356, acc 0.4375\n",
      "2018-07-18T14:23:59.387713: step 382, loss 1.65863, acc 0.4375\n",
      "2018-07-18T14:24:09.790888: step 383, loss 1.35108, acc 0.390625\n",
      "2018-07-18T14:24:20.051445: step 384, loss 1.74216, acc 0.46875\n",
      "2018-07-18T14:24:30.092587: step 385, loss 1.39813, acc 0.53125\n",
      "2018-07-18T14:24:40.237454: step 386, loss 1.2861, acc 0.5\n",
      "2018-07-18T14:24:50.211776: step 387, loss 1.17997, acc 0.625\n",
      "2018-07-18T14:25:00.037495: step 388, loss 1.78121, acc 0.40625\n",
      "2018-07-18T14:25:09.608896: step 389, loss 1.50202, acc 0.5\n",
      "2018-07-18T14:25:19.347847: step 390, loss 1.35843, acc 0.484375\n",
      "2018-07-18T14:25:29.340124: step 391, loss 1.46422, acc 0.484375\n",
      "2018-07-18T14:25:38.977345: step 392, loss 1.49147, acc 0.4375\n",
      "2018-07-18T14:25:48.854927: step 393, loss 1.40349, acc 0.625\n",
      "2018-07-18T14:25:58.591887: step 394, loss 1.41979, acc 0.421875\n",
      "2018-07-18T14:26:08.815538: step 395, loss 1.18542, acc 0.515625\n",
      "2018-07-18T14:26:19.138927: step 396, loss 1.40411, acc 0.5625\n",
      "2018-07-18T14:26:30.380858: step 397, loss 1.02024, acc 0.671875\n",
      "2018-07-18T14:26:40.518743: step 398, loss 1.26552, acc 0.5\n",
      "2018-07-18T14:26:51.072515: step 399, loss 1.49468, acc 0.515625\n",
      "2018-07-18T14:27:02.395231: step 400, loss 1.61745, acc 0.4375\n",
      "\n",
      "Evaluation:\n",
      "2018-07-18T14:28:38.689677: step 400, loss 1.02515, acc 0.602\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\dkdk6\\cnn-text-classification-tf-master\\runs\\1531944623\\checkpoints\\model-400\n",
      "\n",
      "2018-07-18T14:28:49.593513: step 401, loss 1.16438, acc 0.546875\n",
      "2018-07-18T14:28:59.108065: step 402, loss 1.28795, acc 0.515625\n",
      "2018-07-18T14:29:08.727338: step 403, loss 1.23465, acc 0.609375\n",
      "2018-07-18T14:29:18.218949: step 404, loss 1.44891, acc 0.484375\n",
      "2018-07-18T14:29:27.271737: step 405, loss 1.51134, acc 0.5\n",
      "2018-07-18T14:29:37.225122: step 406, loss 1.70303, acc 0.515625\n",
      "2018-07-18T14:29:47.045848: step 407, loss 1.24558, acc 0.46875\n",
      "2018-07-18T14:29:57.069038: step 408, loss 1.27683, acc 0.5625\n",
      "2018-07-18T14:30:06.713244: step 409, loss 1.55319, acc 0.421875\n",
      "2018-07-18T14:30:16.741423: step 410, loss 1.3608, acc 0.453125\n",
      "2018-07-18T14:30:28.184816: step 411, loss 1.41412, acc 0.546875\n",
      "2018-07-18T14:30:39.129542: step 412, loss 1.48862, acc 0.421875\n",
      "2018-07-18T14:30:49.388104: step 413, loss 1.44598, acc 0.390625\n",
      "2018-07-18T14:30:58.936565: step 414, loss 1.40736, acc 0.484375\n",
      "2018-07-18T14:31:08.419203: step 415, loss 1.17369, acc 0.546875\n",
      "2018-07-18T14:31:19.089663: step 416, loss 1.5775, acc 0.421875\n",
      "2018-07-18T14:31:28.784732: step 417, loss 1.4805, acc 0.421875\n",
      "2018-07-18T14:31:38.636382: step 418, loss 1.55792, acc 0.421875\n",
      "2018-07-18T14:31:48.278592: step 419, loss 1.28493, acc 0.578125\n",
      "2018-07-18T14:31:57.959700: step 420, loss 1.33703, acc 0.484375\n",
      "2018-07-18T14:32:08.086613: step 421, loss 1.28122, acc 0.546875\n",
      "2018-07-18T14:32:18.063926: step 422, loss 1.5126, acc 0.484375\n",
      "2018-07-18T14:32:24.616401: step 423, loss 1.33265, acc 0.55\n",
      "2018-07-18T14:32:34.813130: step 424, loss 1.43564, acc 0.5\n",
      "2018-07-18T14:32:44.978939: step 425, loss 1.39448, acc 0.359375\n",
      "2018-07-18T14:32:54.143427: step 426, loss 1.34666, acc 0.625\n",
      "2018-07-18T14:33:03.492475: step 427, loss 1.09361, acc 0.53125\n",
      "2018-07-18T14:33:13.068861: step 428, loss 1.4688, acc 0.53125\n",
      "2018-07-18T14:33:21.961077: step 429, loss 1.43283, acc 0.5\n",
      "2018-07-18T14:33:33.483291: step 430, loss 1.1103, acc 0.5625\n",
      "2018-07-18T14:33:42.433352: step 431, loss 1.22564, acc 0.578125\n",
      "2018-07-18T14:33:52.373765: step 432, loss 1.51305, acc 0.375\n",
      "2018-07-18T14:34:01.750686: step 433, loss 1.46212, acc 0.53125\n",
      "2018-07-18T14:34:11.450742: step 434, loss 1.16708, acc 0.609375\n",
      "2018-07-18T14:34:20.444686: step 435, loss 1.112, acc 0.609375\n",
      "2018-07-18T14:34:30.234501: step 436, loss 1.32025, acc 0.421875\n",
      "2018-07-18T14:34:39.390013: step 437, loss 1.26153, acc 0.5\n",
      "2018-07-18T14:34:49.366331: step 438, loss 1.25342, acc 0.546875\n",
      "2018-07-18T14:34:59.170110: step 439, loss 1.32023, acc 0.484375\n",
      "2018-07-18T14:35:09.594230: step 440, loss 1.35191, acc 0.546875\n",
      "2018-07-18T14:35:19.865756: step 441, loss 1.01675, acc 0.640625\n",
      "2018-07-18T14:35:30.980030: step 442, loss 1.32688, acc 0.515625\n",
      "2018-07-18T14:35:40.995242: step 443, loss 1.57133, acc 0.453125\n",
      "2018-07-18T14:35:50.991503: step 444, loss 1.55356, acc 0.5625\n",
      "2018-07-18T14:36:01.087501: step 445, loss 1.13842, acc 0.5625\n",
      "2018-07-18T14:36:10.814485: step 446, loss 1.31337, acc 0.546875\n",
      "2018-07-18T14:36:20.239277: step 447, loss 1.37914, acc 0.5625\n",
      "2018-07-18T14:36:30.617520: step 448, loss 1.37959, acc 0.5625\n",
      "2018-07-18T14:36:40.396363: step 449, loss 1.46236, acc 0.421875\n",
      "2018-07-18T14:36:50.410579: step 450, loss 1.48038, acc 0.546875\n",
      "2018-07-18T14:37:00.630245: step 451, loss 1.15791, acc 0.46875\n",
      "2018-07-18T14:37:11.255825: step 452, loss 1.16862, acc 0.625\n",
      "2018-07-18T14:37:20.965853: step 453, loss 1.03071, acc 0.59375\n",
      "2018-07-18T14:37:31.344099: step 454, loss 1.12058, acc 0.546875\n",
      "2018-07-18T14:37:41.627592: step 455, loss 1.2897, acc 0.5\n",
      "2018-07-18T14:37:51.595930: step 456, loss 1.1532, acc 0.578125\n",
      "2018-07-18T14:38:02.183613: step 457, loss 1.1167, acc 0.578125\n",
      "2018-07-18T14:38:12.339448: step 458, loss 1.38478, acc 0.546875\n",
      "2018-07-18T14:38:22.231989: step 459, loss 1.08665, acc 0.578125\n",
      "2018-07-18T14:38:32.528449: step 460, loss 1.35422, acc 0.546875\n",
      "2018-07-18T14:38:44.131415: step 461, loss 1.22212, acc 0.609375\n",
      "2018-07-18T14:38:54.237386: step 462, loss 1.09544, acc 0.53125\n",
      "2018-07-18T14:39:04.532849: step 463, loss 0.988316, acc 0.65625\n",
      "2018-07-18T14:39:14.338622: step 464, loss 1.05559, acc 0.546875\n",
      "2018-07-18T14:39:25.524703: step 465, loss 1.22779, acc 0.546875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-18T14:39:36.155270: step 466, loss 1.06431, acc 0.59375\n",
      "2018-07-18T14:39:47.297468: step 467, loss 1.46484, acc 0.53125\n",
      "2018-07-18T14:40:02.492828: step 468, loss 1.33794, acc 0.5\n",
      "2018-07-18T14:40:17.680206: step 469, loss 1.57196, acc 0.46875\n",
      "2018-07-18T14:40:28.082383: step 470, loss 1.28024, acc 0.5625\n",
      "2018-07-18T14:40:39.112879: step 471, loss 1.46251, acc 0.515625\n",
      "2018-07-18T14:40:50.372764: step 472, loss 1.32163, acc 0.609375\n",
      "2018-07-18T14:41:02.918211: step 473, loss 1.24274, acc 0.515625\n",
      "2018-07-18T14:41:12.432761: step 474, loss 1.03421, acc 0.625\n",
      "2018-07-18T14:41:21.971249: step 475, loss 1.18015, acc 0.625\n",
      "2018-07-18T14:41:31.468847: step 476, loss 1.48505, acc 0.46875\n",
      "2018-07-18T14:41:43.102730: step 477, loss 1.41187, acc 0.46875\n",
      "2018-07-18T14:41:55.055777: step 478, loss 1.17427, acc 0.609375\n",
      "2018-07-18T14:42:06.749483: step 479, loss 1.08457, acc 0.609375\n",
      "2018-07-18T14:42:17.707176: step 480, loss 1.13958, acc 0.46875\n",
      "2018-07-18T14:42:30.535862: step 481, loss 1.32763, acc 0.46875\n",
      "2018-07-18T14:42:46.872168: step 482, loss 1.18781, acc 0.59375\n",
      "2018-07-18T14:43:04.013541: step 483, loss 1.19954, acc 0.5625\n",
      "2018-07-18T14:43:14.584269: step 484, loss 1.3222, acc 0.453125\n",
      "2018-07-18T14:43:25.172649: step 485, loss 1.37627, acc 0.515625\n",
      "2018-07-18T14:43:36.068507: step 486, loss 1.06433, acc 0.609375\n",
      "2018-07-18T14:43:50.701369: step 487, loss 1.16384, acc 0.59375\n",
      "2018-07-18T14:44:03.689630: step 488, loss 1.43909, acc 0.53125\n",
      "2018-07-18T14:44:15.024313: step 489, loss 1.05763, acc 0.671875\n",
      "2018-07-18T14:44:27.189776: step 490, loss 1.3459, acc 0.578125\n",
      "2018-07-18T14:44:38.970267: step 491, loss 1.26073, acc 0.5\n",
      "2018-07-18T14:44:51.128747: step 492, loss 0.974768, acc 0.5625\n",
      "2018-07-18T14:45:01.227735: step 493, loss 1.17112, acc 0.546875\n",
      "2018-07-18T14:45:11.503252: step 494, loss 1.43144, acc 0.40625\n",
      "2018-07-18T14:45:22.041067: step 495, loss 1.15504, acc 0.546875\n",
      "2018-07-18T14:45:32.167980: step 496, loss 1.08893, acc 0.578125\n",
      "2018-07-18T14:45:42.153274: step 497, loss 1.12715, acc 0.578125\n",
      "2018-07-18T14:45:51.490301: step 498, loss 1.19297, acc 0.5625\n",
      "2018-07-18T14:46:01.195345: step 499, loss 0.995504, acc 0.6875\n",
      "2018-07-18T14:46:10.663020: step 500, loss 1.07155, acc 0.609375\n",
      "\n",
      "Evaluation:\n",
      "2018-07-18T14:47:52.199446: step 500, loss 1.00255, acc 0.621\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\dkdk6\\cnn-text-classification-tf-master\\runs\\1531944623\\checkpoints\\model-500\n",
      "\n",
      "2018-07-18T14:48:06.286765: step 501, loss 1.33431, acc 0.515625\n",
      "2018-07-18T14:48:16.145398: step 502, loss 1.31617, acc 0.5\n",
      "2018-07-18T14:48:25.424597: step 503, loss 1.36397, acc 0.515625\n",
      "2018-07-18T14:48:34.759612: step 504, loss 1.34577, acc 0.453125\n",
      "2018-07-18T14:48:44.049764: step 505, loss 0.969368, acc 0.640625\n",
      "2018-07-18T14:48:53.497495: step 506, loss 1.21629, acc 0.53125\n",
      "2018-07-18T14:49:04.609774: step 507, loss 1.30107, acc 0.5625\n",
      "2018-07-18T14:49:14.032570: step 508, loss 1.29537, acc 0.59375\n",
      "2018-07-18T14:49:23.208030: step 509, loss 1.30841, acc 0.515625\n",
      "2018-07-18T14:49:32.741536: step 510, loss 1.13527, acc 0.578125\n",
      "2018-07-18T14:49:42.073571: step 511, loss 1.075, acc 0.609375\n",
      "2018-07-18T14:49:53.761311: step 512, loss 1.01657, acc 0.59375\n",
      "2018-07-18T14:50:04.334031: step 513, loss 1.04172, acc 0.59375\n",
      "2018-07-18T14:50:16.575291: step 514, loss 1.07322, acc 0.59375\n",
      "2018-07-18T14:50:27.327532: step 515, loss 1.40915, acc 0.515625\n",
      "2018-07-18T14:50:36.884969: step 516, loss 1.52939, acc 0.375\n",
      "2018-07-18T14:50:48.174935: step 517, loss 1.28625, acc 0.515625\n",
      "2018-07-18T14:50:59.048852: step 518, loss 0.920249, acc 0.671875\n",
      "2018-07-18T14:51:10.263856: step 519, loss 1.26896, acc 0.515625\n",
      "2018-07-18T14:51:21.932646: step 520, loss 1.07867, acc 0.53125\n",
      "2018-07-18T14:51:33.677234: step 521, loss 1.23531, acc 0.609375\n",
      "2018-07-18T14:51:44.025557: step 522, loss 1.25333, acc 0.53125\n",
      "2018-07-18T14:51:54.662108: step 523, loss 1.03542, acc 0.578125\n",
      "2018-07-18T14:52:04.920667: step 524, loss 0.995025, acc 0.640625\n",
      "2018-07-18T14:52:15.144323: step 525, loss 0.97847, acc 0.59375\n",
      "2018-07-18T14:52:25.405876: step 526, loss 1.51501, acc 0.421875\n",
      "2018-07-18T14:52:36.769485: step 527, loss 1.11826, acc 0.515625\n",
      "2018-07-18T14:52:47.050983: step 528, loss 1.25359, acc 0.5\n",
      "2018-07-18T14:52:57.026304: step 529, loss 1.17607, acc 0.65625\n",
      "2018-07-18T14:53:06.948765: step 530, loss 1.19185, acc 0.625\n",
      "2018-07-18T14:53:17.185384: step 531, loss 1.1641, acc 0.625\n",
      "2018-07-18T14:53:27.231514: step 532, loss 1.2147, acc 0.546875\n",
      "2018-07-18T14:53:37.123058: step 533, loss 1.21209, acc 0.578125\n",
      "2018-07-18T14:53:47.133286: step 534, loss 1.0521, acc 0.59375\n",
      "2018-07-18T14:53:56.888193: step 535, loss 1.03495, acc 0.59375\n",
      "2018-07-18T14:54:07.563639: step 536, loss 1.05904, acc 0.546875\n",
      "2018-07-18T14:54:19.343134: step 537, loss 1.34267, acc 0.515625\n",
      "2018-07-18T14:54:29.620645: step 538, loss 1.1776, acc 0.609375\n",
      "2018-07-18T14:54:41.263504: step 539, loss 1.30224, acc 0.453125\n",
      "2018-07-18T14:54:52.630103: step 540, loss 1.08741, acc 0.6875\n",
      "2018-07-18T14:55:02.940527: step 541, loss 1.17398, acc 0.546875\n",
      "2018-07-18T14:55:15.172809: step 542, loss 0.919608, acc 0.671875\n",
      "2018-07-18T14:55:26.720759: step 543, loss 1.23127, acc 0.546875\n",
      "2018-07-18T14:55:38.658544: step 544, loss 1.08, acc 0.546875\n",
      "2018-07-18T14:55:51.610904: step 545, loss 1.10155, acc 0.625\n",
      "2018-07-18T14:56:04.934267: step 546, loss 1.24681, acc 0.484375\n",
      "2018-07-18T14:56:16.141292: step 547, loss 1.06474, acc 0.53125\n",
      "2018-07-18T14:56:26.183433: step 548, loss 1.32555, acc 0.578125\n",
      "2018-07-18T14:56:36.375174: step 549, loss 1.17239, acc 0.515625\n",
      "2018-07-18T14:56:45.656350: step 550, loss 1.00379, acc 0.609375\n",
      "2018-07-18T14:56:55.962783: step 551, loss 1.55385, acc 0.46875\n",
      "2018-07-18T14:57:06.116625: step 552, loss 0.961301, acc 0.625\n",
      "2018-07-18T14:57:15.869538: step 553, loss 1.27994, acc 0.484375\n",
      "2018-07-18T14:57:25.599515: step 554, loss 1.13322, acc 0.59375\n",
      "2018-07-18T14:57:35.756348: step 555, loss 1.16771, acc 0.578125\n",
      "2018-07-18T14:57:46.484655: step 556, loss 1.26961, acc 0.546875\n",
      "2018-07-18T14:57:56.882843: step 557, loss 1.13481, acc 0.515625\n",
      "2018-07-18T14:58:07.149384: step 558, loss 1.14704, acc 0.609375\n",
      "2018-07-18T14:58:16.872379: step 559, loss 1.321, acc 0.390625\n",
      "2018-07-18T14:58:26.426822: step 560, loss 1.21487, acc 0.5625\n",
      "2018-07-18T14:58:36.788116: step 561, loss 1.41498, acc 0.546875\n",
      "2018-07-18T14:58:47.597202: step 562, loss 1.11779, acc 0.578125\n",
      "2018-07-18T14:58:58.168922: step 563, loss 1.23609, acc 0.609375\n",
      "2018-07-18T14:59:04.945798: step 564, loss 1.20574, acc 0.6\n",
      "2018-07-18T14:59:15.260210: step 565, loss 1.08524, acc 0.640625\n",
      "2018-07-18T14:59:26.456265: step 566, loss 1.0433, acc 0.59375\n",
      "2018-07-18T14:59:36.244086: step 567, loss 1.16264, acc 0.53125\n",
      "2018-07-18T14:59:45.364691: step 568, loss 0.976597, acc 0.640625\n",
      "2018-07-18T14:59:55.155505: step 569, loss 0.988444, acc 0.609375\n",
      "2018-07-18T15:00:04.995186: step 570, loss 1.03717, acc 0.65625\n",
      "2018-07-18T15:00:14.555617: step 571, loss 1.18684, acc 0.546875\n",
      "2018-07-18T15:00:24.317505: step 572, loss 1.12781, acc 0.609375\n",
      "2018-07-18T15:00:33.245626: step 573, loss 1.05569, acc 0.5625\n",
      "2018-07-18T15:00:42.912771: step 574, loss 1.09106, acc 0.546875\n",
      "2018-07-18T15:00:52.862160: step 575, loss 1.12435, acc 0.5625\n",
      "2018-07-18T15:01:02.621057: step 576, loss 1.07348, acc 0.5625\n",
      "2018-07-18T15:01:12.133613: step 577, loss 0.986283, acc 0.640625\n",
      "2018-07-18T15:01:21.461667: step 578, loss 0.94667, acc 0.703125\n",
      "2018-07-18T15:01:32.358521: step 579, loss 0.932323, acc 0.65625\n",
      "2018-07-18T15:01:44.594792: step 580, loss 0.865681, acc 0.703125\n",
      "2018-07-18T15:01:56.225685: step 581, loss 0.87385, acc 0.59375\n",
      "2018-07-18T15:02:07.149977: step 582, loss 1.2072, acc 0.5\n",
      "2018-07-18T15:02:18.260261: step 583, loss 0.991373, acc 0.671875\n",
      "2018-07-18T15:02:27.939373: step 584, loss 1.14895, acc 0.578125\n",
      "2018-07-18T15:02:37.707248: step 585, loss 1.07053, acc 0.609375\n",
      "2018-07-18T15:02:47.318542: step 586, loss 0.985389, acc 0.578125\n",
      "2018-07-18T15:02:56.803172: step 587, loss 1.21592, acc 0.5625\n",
      "2018-07-18T15:03:07.191387: step 588, loss 1.28236, acc 0.5\n",
      "2018-07-18T15:03:17.108862: step 589, loss 1.12026, acc 0.5625\n",
      "2018-07-18T15:03:27.030327: step 590, loss 0.962586, acc 0.640625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-18T15:03:36.890951: step 591, loss 0.868869, acc 0.734375\n",
      "2018-07-18T15:03:46.712681: step 592, loss 1.22672, acc 0.5625\n",
      "2018-07-18T15:03:56.772774: step 593, loss 1.07267, acc 0.609375\n",
      "2018-07-18T15:04:07.241775: step 594, loss 1.33421, acc 0.5\n",
      "2018-07-18T15:04:18.029920: step 595, loss 1.21309, acc 0.546875\n",
      "2018-07-18T15:04:29.209018: step 596, loss 0.943583, acc 0.65625\n",
      "2018-07-18T15:04:39.685000: step 597, loss 1.12858, acc 0.5625\n",
      "2018-07-18T15:04:50.303034: step 598, loss 1.05758, acc 0.703125\n",
      "2018-07-18T15:05:00.321239: step 599, loss 0.942707, acc 0.640625\n",
      "2018-07-18T15:05:09.918570: step 600, loss 0.893558, acc 0.65625\n",
      "\n",
      "Evaluation:\n",
      "2018-07-18T15:06:53.743737: step 600, loss 0.924855, acc 0.677\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\dkdk6\\cnn-text-classification-tf-master\\runs\\1531944623\\checkpoints\\model-600\n",
      "\n",
      "2018-07-18T15:07:04.054160: step 601, loss 0.998053, acc 0.578125\n",
      "2018-07-18T15:07:13.201693: step 602, loss 1.05204, acc 0.578125\n",
      "2018-07-18T15:07:21.918380: step 603, loss 1.16922, acc 0.59375\n",
      "2018-07-18T15:07:29.971839: step 604, loss 1.06133, acc 0.625\n",
      "2018-07-18T15:07:38.920903: step 605, loss 1.09765, acc 0.625\n",
      "2018-07-18T15:07:47.705408: step 606, loss 1.22992, acc 0.484375\n",
      "2018-07-18T15:07:56.622558: step 607, loss 1.13823, acc 0.53125\n",
      "2018-07-18T15:08:05.438976: step 608, loss 1.09471, acc 0.609375\n",
      "2018-07-18T15:08:14.397018: step 609, loss 1.20304, acc 0.609375\n",
      "2018-07-18T15:08:25.067545: step 610, loss 0.948365, acc 0.625\n",
      "2018-07-18T15:08:35.893589: step 611, loss 1.13447, acc 0.578125\n",
      "2018-07-18T15:08:47.301382: step 612, loss 1.14673, acc 0.53125\n",
      "2018-07-18T15:08:57.013405: step 613, loss 0.896389, acc 0.578125\n",
      "2018-07-18T15:09:07.696441: step 614, loss 1.2099, acc 0.578125\n",
      "2018-07-18T15:09:18.054736: step 615, loss 1.34213, acc 0.5625\n",
      "2018-07-18T15:09:29.332572: step 616, loss 1.47215, acc 0.453125\n",
      "2018-07-18T15:09:41.165922: step 617, loss 1.15555, acc 0.609375\n",
      "2018-07-18T15:09:53.439096: step 618, loss 1.19734, acc 0.609375\n",
      "2018-07-18T15:10:04.233225: step 619, loss 0.82537, acc 0.6875\n",
      "2018-07-18T15:10:14.159675: step 620, loss 0.952622, acc 0.640625\n",
      "2018-07-18T15:10:25.244029: step 621, loss 1.32908, acc 0.46875\n",
      "2018-07-18T15:10:36.013224: step 622, loss 1.1583, acc 0.625\n",
      "2018-07-18T15:10:47.431686: step 623, loss 1.17127, acc 0.546875\n",
      "2018-07-18T15:10:58.422288: step 624, loss 1.00624, acc 0.5625\n",
      "2018-07-18T15:11:09.240355: step 625, loss 1.26279, acc 0.59375\n",
      "2018-07-18T15:11:19.168798: step 626, loss 1.1173, acc 0.53125\n",
      "2018-07-18T15:11:30.651087: step 627, loss 1.00196, acc 0.625\n",
      "2018-07-18T15:11:42.582176: step 628, loss 1.17652, acc 0.53125\n",
      "2018-07-18T15:11:53.399244: step 629, loss 1.11069, acc 0.578125\n",
      "2018-07-18T15:12:05.333325: step 630, loss 0.969675, acc 0.703125\n",
      "2018-07-18T15:12:17.041010: step 631, loss 0.94173, acc 0.65625\n",
      "2018-07-18T15:12:29.725561: step 632, loss 1.13133, acc 0.5625\n",
      "2018-07-18T15:12:40.080864: step 633, loss 1.06816, acc 0.609375\n",
      "2018-07-18T15:12:51.341744: step 634, loss 1.00946, acc 0.609375\n",
      "2018-07-18T15:13:02.806082: step 635, loss 1.38572, acc 0.515625\n",
      "2018-07-18T15:13:14.382120: step 636, loss 1.05328, acc 0.5625\n",
      "2018-07-18T15:13:26.903630: step 637, loss 1.13665, acc 0.609375\n",
      "2018-07-18T15:13:38.597352: step 638, loss 1.1843, acc 0.59375\n",
      "2018-07-18T15:13:50.863545: step 639, loss 1.22837, acc 0.609375\n",
      "2018-07-18T15:14:02.212191: step 640, loss 0.889665, acc 0.640625\n",
      "2018-07-18T15:14:13.334442: step 641, loss 1.153, acc 0.6875\n",
      "2018-07-18T15:14:23.969997: step 642, loss 1.00012, acc 0.640625\n",
      "2018-07-18T15:14:35.883133: step 643, loss 1.36921, acc 0.46875\n",
      "2018-07-18T15:14:49.311223: step 644, loss 1.37146, acc 0.46875\n",
      "2018-07-18T15:15:02.768225: step 645, loss 1.20281, acc 0.59375\n",
      "2018-07-18T15:15:15.932016: step 646, loss 0.922475, acc 0.625\n",
      "2018-07-18T15:15:28.115430: step 647, loss 1.07149, acc 0.640625\n",
      "2018-07-18T15:15:40.163208: step 648, loss 0.979843, acc 0.625\n",
      "2018-07-18T15:15:51.536786: step 649, loss 0.916283, acc 0.625\n",
      "2018-07-18T15:16:02.426658: step 650, loss 1.05655, acc 0.546875\n",
      "2018-07-18T15:16:13.163952: step 651, loss 1.37369, acc 0.375\n",
      "2018-07-18T15:16:24.632267: step 652, loss 1.21549, acc 0.53125\n",
      "2018-07-18T15:16:34.760178: step 653, loss 1.22265, acc 0.59375\n",
      "2018-07-18T15:16:45.213219: step 654, loss 0.955469, acc 0.65625\n",
      "2018-07-18T15:16:55.679226: step 655, loss 1.19218, acc 0.5625\n",
      "2018-07-18T15:17:05.367315: step 656, loss 0.811558, acc 0.65625\n",
      "2018-07-18T15:17:15.459321: step 657, loss 1.4233, acc 0.46875\n",
      "2018-07-18T15:17:25.631115: step 658, loss 1.01924, acc 0.546875\n",
      "2018-07-18T15:17:35.433897: step 659, loss 1.12725, acc 0.546875\n",
      "2018-07-18T15:17:45.866993: step 660, loss 0.884646, acc 0.59375\n",
      "2018-07-18T15:17:55.684733: step 661, loss 1.07133, acc 0.625\n",
      "2018-07-18T15:18:05.653071: step 662, loss 1.24573, acc 0.546875\n",
      "2018-07-18T15:18:15.623404: step 663, loss 1.19193, acc 0.578125\n",
      "2018-07-18T15:18:25.499988: step 664, loss 1.28118, acc 0.5\n",
      "2018-07-18T15:18:35.523179: step 665, loss 1.07254, acc 0.625\n",
      "2018-07-18T15:18:45.556342: step 666, loss 1.09431, acc 0.609375\n",
      "2018-07-18T15:18:56.303599: step 667, loss 0.89793, acc 0.640625\n",
      "2018-07-18T15:19:07.505638: step 668, loss 0.927966, acc 0.65625\n",
      "2018-07-18T15:19:18.235937: step 669, loss 1.23358, acc 0.5625\n",
      "2018-07-18T15:19:29.514770: step 670, loss 0.979445, acc 0.59375\n",
      "2018-07-18T15:19:41.396297: step 671, loss 1.1653, acc 0.59375\n",
      "2018-07-18T15:19:53.519869: step 672, loss 0.995086, acc 0.5625\n",
      "2018-07-18T15:20:07.550345: step 673, loss 1.18022, acc 0.484375\n",
      "2018-07-18T15:20:19.047594: step 674, loss 1.19149, acc 0.546875\n",
      "2018-07-18T15:20:30.621637: step 675, loss 1.09653, acc 0.5625\n",
      "2018-07-18T15:20:41.868554: step 676, loss 0.931565, acc 0.65625\n",
      "2018-07-18T15:20:56.432601: step 677, loss 0.961392, acc 0.65625\n",
      "2018-07-18T15:21:08.606042: step 678, loss 1.14269, acc 0.5625\n",
      "2018-07-18T15:21:17.626914: step 679, loss 1.46025, acc 0.46875\n",
      "2018-07-18T15:21:26.689674: step 680, loss 0.878825, acc 0.65625\n",
      "2018-07-18T15:21:35.341533: step 681, loss 1.23165, acc 0.578125\n",
      "2018-07-18T15:21:43.926571: step 682, loss 1.09795, acc 0.640625\n",
      "2018-07-18T15:21:54.403550: step 683, loss 1.29327, acc 0.578125\n",
      "2018-07-18T15:22:03.738580: step 684, loss 1.16602, acc 0.515625\n",
      "2018-07-18T15:22:12.244830: step 685, loss 0.962337, acc 0.671875\n",
      "2018-07-18T15:22:20.750082: step 686, loss 0.886596, acc 0.671875\n",
      "2018-07-18T15:22:29.460783: step 687, loss 1.06433, acc 0.671875\n",
      "2018-07-18T15:22:38.067762: step 688, loss 1.11445, acc 0.578125\n",
      "2018-07-18T15:22:46.511180: step 689, loss 1.14178, acc 0.53125\n",
      "2018-07-18T15:22:55.163039: step 690, loss 1.15166, acc 0.59375\n",
      "2018-07-18T15:23:03.914631: step 691, loss 1.09406, acc 0.546875\n",
      "2018-07-18T15:23:12.760970: step 692, loss 0.868361, acc 0.640625\n",
      "2018-07-18T15:23:22.530839: step 693, loss 1.26927, acc 0.515625\n",
      "2018-07-18T15:23:31.300384: step 694, loss 1.01545, acc 0.640625\n",
      "2018-07-18T15:23:40.020061: step 695, loss 0.850525, acc 0.640625\n",
      "2018-07-18T15:23:48.693862: step 696, loss 1.09964, acc 0.6875\n",
      "2018-07-18T15:23:57.448447: step 697, loss 1.19535, acc 0.65625\n",
      "2018-07-18T15:24:06.130227: step 698, loss 1.19014, acc 0.5625\n",
      "2018-07-18T15:24:14.897775: step 699, loss 0.866989, acc 0.703125\n",
      "2018-07-18T15:24:23.668318: step 700, loss 0.873097, acc 0.640625\n",
      "\n",
      "Evaluation:\n",
      "2018-07-18T15:25:55.671242: step 700, loss 0.93419, acc 0.66\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\dkdk6\\cnn-text-classification-tf-master\\runs\\1531944623\\checkpoints\\model-700\n",
      "\n",
      "2018-07-18T15:26:07.685626: step 701, loss 1.05907, acc 0.640625\n",
      "2018-07-18T15:26:16.765340: step 702, loss 0.981327, acc 0.640625\n",
      "2018-07-18T15:26:26.568121: step 703, loss 1.19288, acc 0.578125\n",
      "2018-07-18T15:26:36.566380: step 704, loss 1.00644, acc 0.53125\n",
      "2018-07-18T15:26:42.813671: step 705, loss 1.09286, acc 0.675\n",
      "2018-07-18T15:26:52.039993: step 706, loss 0.959314, acc 0.65625\n",
      "2018-07-18T15:27:01.415916: step 707, loss 0.970135, acc 0.59375\n",
      "2018-07-18T15:27:11.809117: step 708, loss 1.1318, acc 0.53125\n",
      "2018-07-18T15:27:22.439685: step 709, loss 1.03374, acc 0.625\n",
      "2018-07-18T15:27:35.026021: step 710, loss 0.922856, acc 0.59375\n",
      "2018-07-18T15:27:46.090427: step 711, loss 0.958153, acc 0.625\n",
      "2018-07-18T15:27:58.869249: step 712, loss 0.884622, acc 0.671875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-18T15:28:09.424018: step 713, loss 0.872716, acc 0.625\n",
      "2018-07-18T15:28:20.227123: step 714, loss 0.938736, acc 0.640625\n",
      "2018-07-18T15:28:30.444793: step 715, loss 0.803539, acc 0.671875\n",
      "2018-07-18T15:28:40.536803: step 716, loss 1.17961, acc 0.609375\n",
      "2018-07-18T15:28:50.830271: step 717, loss 0.835206, acc 0.65625\n",
      "2018-07-18T15:29:01.445878: step 718, loss 0.798445, acc 0.671875\n",
      "2018-07-18T15:29:11.146931: step 719, loss 1.17164, acc 0.5625\n",
      "2018-07-18T15:29:21.541989: step 720, loss 0.783042, acc 0.734375\n",
      "2018-07-18T15:29:30.278622: step 721, loss 0.901427, acc 0.609375\n",
      "2018-07-18T15:29:39.713387: step 722, loss 0.991325, acc 0.625\n",
      "2018-07-18T15:29:48.355273: step 723, loss 0.91597, acc 0.671875\n",
      "2018-07-18T15:29:56.845564: step 724, loss 0.934484, acc 0.671875\n",
      "2018-07-18T15:30:05.370763: step 725, loss 0.814985, acc 0.71875\n",
      "2018-07-18T15:30:13.947822: step 726, loss 0.978145, acc 0.65625\n",
      "2018-07-18T15:30:24.896538: step 727, loss 0.719181, acc 0.75\n",
      "2018-07-18T15:30:35.366535: step 728, loss 1.13395, acc 0.546875\n",
      "2018-07-18T15:30:44.554958: step 729, loss 1.14346, acc 0.59375\n",
      "2018-07-18T15:30:53.186871: step 730, loss 1.01651, acc 0.65625\n",
      "2018-07-18T15:31:01.877648: step 731, loss 0.94824, acc 0.640625\n",
      "2018-07-18T15:31:10.371908: step 732, loss 0.848422, acc 0.6875\n",
      "2018-07-18T15:31:19.136465: step 733, loss 1.07799, acc 0.546875\n",
      "2018-07-18T15:31:27.783338: step 734, loss 0.960941, acc 0.640625\n",
      "2018-07-18T15:31:36.449159: step 735, loss 1.00735, acc 0.59375\n",
      "2018-07-18T15:31:45.036192: step 736, loss 0.841659, acc 0.6875\n",
      "2018-07-18T15:31:53.719966: step 737, loss 0.930357, acc 0.546875\n",
      "2018-07-18T15:32:02.578273: step 738, loss 0.856291, acc 0.671875\n",
      "2018-07-18T15:32:11.611113: step 739, loss 1.05034, acc 0.546875\n",
      "2018-07-18T15:32:20.220086: step 740, loss 0.780329, acc 0.6875\n",
      "2018-07-18T15:32:28.736309: step 741, loss 1.00269, acc 0.65625\n",
      "2018-07-18T15:32:37.272478: step 742, loss 0.86536, acc 0.71875\n",
      "2018-07-18T15:32:45.763767: step 743, loss 0.913371, acc 0.609375\n",
      "2018-07-18T15:32:54.365760: step 744, loss 1.1777, acc 0.65625\n",
      "2018-07-18T15:33:03.265955: step 745, loss 0.981486, acc 0.59375\n",
      "2018-07-18T15:33:11.894875: step 746, loss 1.31049, acc 0.453125\n",
      "2018-07-18T15:33:20.578649: step 747, loss 1.00144, acc 0.640625\n",
      "2018-07-18T15:33:29.762087: step 748, loss 0.865101, acc 0.6875\n",
      "2018-07-18T15:33:38.357098: step 749, loss 0.929479, acc 0.6875\n",
      "2018-07-18T15:33:46.721726: step 750, loss 1.09341, acc 0.546875\n",
      "2018-07-18T15:33:55.266870: step 751, loss 0.965681, acc 0.65625\n",
      "2018-07-18T15:34:04.015471: step 752, loss 0.897715, acc 0.75\n",
      "2018-07-18T15:34:12.474845: step 753, loss 1.07266, acc 0.609375\n",
      "2018-07-18T15:34:21.199509: step 754, loss 0.823921, acc 0.703125\n",
      "2018-07-18T15:34:30.293699: step 755, loss 0.75806, acc 0.734375\n",
      "2018-07-18T15:34:40.860670: step 756, loss 0.946282, acc 0.671875\n",
      "2018-07-18T15:34:53.324334: step 757, loss 0.888986, acc 0.6875\n",
      "2018-07-18T15:35:03.467206: step 758, loss 1.16478, acc 0.546875\n",
      "2018-07-18T15:35:14.407943: step 759, loss 1.05498, acc 0.640625\n",
      "2018-07-18T15:35:23.224362: step 760, loss 0.923018, acc 0.59375\n",
      "2018-07-18T15:35:34.807381: step 761, loss 0.833533, acc 0.671875\n",
      "2018-07-18T15:35:45.759089: step 762, loss 1.07229, acc 0.625\n",
      "2018-07-18T15:35:58.000347: step 763, loss 1.10752, acc 0.671875\n",
      "2018-07-18T15:36:09.547464: step 764, loss 0.784505, acc 0.65625\n",
      "2018-07-18T15:36:21.208275: step 765, loss 0.975074, acc 0.578125\n",
      "2018-07-18T15:36:30.758731: step 766, loss 0.955366, acc 0.65625\n",
      "2018-07-18T15:36:39.711784: step 767, loss 1.00332, acc 0.671875\n",
      "2018-07-18T15:36:49.258250: step 768, loss 1.23479, acc 0.53125\n",
      "2018-07-18T15:36:59.728750: step 769, loss 1.11455, acc 0.640625\n",
      "2018-07-18T15:37:10.622613: step 770, loss 0.906238, acc 0.578125\n",
      "2018-07-18T15:37:21.809693: step 771, loss 1.10601, acc 0.578125\n",
      "2018-07-18T15:37:32.880083: step 772, loss 1.03258, acc 0.578125\n",
      "2018-07-18T15:37:44.152931: step 773, loss 1.02423, acc 0.65625\n",
      "2018-07-18T15:37:55.381898: step 774, loss 0.932013, acc 0.640625\n",
      "2018-07-18T15:38:05.603558: step 775, loss 0.898677, acc 0.5625\n",
      "2018-07-18T15:38:15.779342: step 776, loss 1.06089, acc 0.625\n",
      "2018-07-18T15:38:27.457108: step 777, loss 0.893624, acc 0.609375\n",
      "2018-07-18T15:38:38.173445: step 778, loss 0.971402, acc 0.59375\n",
      "2018-07-18T15:38:50.281061: step 779, loss 0.989475, acc 0.625\n",
      "2018-07-18T15:39:01.943867: step 780, loss 1.17176, acc 0.640625\n",
      "2018-07-18T15:39:12.986333: step 781, loss 1.04135, acc 0.671875\n",
      "2018-07-18T15:39:24.741893: step 782, loss 1.11812, acc 0.578125\n",
      "2018-07-18T15:39:35.360490: step 783, loss 0.70468, acc 0.75\n",
      "2018-07-18T15:39:46.478752: step 784, loss 0.815783, acc 0.640625\n",
      "2018-07-18T15:39:57.669820: step 785, loss 1.12923, acc 0.609375\n",
      "2018-07-18T15:40:09.060355: step 786, loss 0.766118, acc 0.71875\n",
      "2018-07-18T15:40:21.724482: step 787, loss 1.07441, acc 0.59375\n",
      "2018-07-18T15:40:33.925847: step 788, loss 0.896802, acc 0.671875\n",
      "2018-07-18T15:40:46.872222: step 789, loss 1.0906, acc 0.546875\n",
      "2018-07-18T15:40:58.457235: step 790, loss 0.942023, acc 0.609375\n",
      "2018-07-18T15:41:09.591455: step 791, loss 0.911987, acc 0.640625\n",
      "2018-07-18T15:41:20.556129: step 792, loss 0.836273, acc 0.609375\n",
      "2018-07-18T15:41:31.975584: step 793, loss 0.945625, acc 0.609375\n",
      "2018-07-18T15:41:42.236142: step 794, loss 0.879805, acc 0.6875\n",
      "2018-07-18T15:41:52.654276: step 795, loss 1.16473, acc 0.59375\n",
      "2018-07-18T15:42:03.323739: step 796, loss 1.0101, acc 0.609375\n",
      "2018-07-18T15:42:12.988889: step 797, loss 0.926867, acc 0.640625\n",
      "2018-07-18T15:42:23.138741: step 798, loss 0.788901, acc 0.6875\n",
      "2018-07-18T15:42:35.291262: step 799, loss 1.09904, acc 0.53125\n",
      "2018-07-18T15:42:45.656538: step 800, loss 0.912956, acc 0.625\n",
      "\n",
      "Evaluation:\n",
      "2018-07-18T15:44:37.420351: step 800, loss 0.881478, acc 0.69\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\dkdk6\\cnn-text-classification-tf-master\\runs\\1531944623\\checkpoints\\model-800\n",
      "\n",
      "2018-07-18T15:44:49.620729: step 801, loss 0.88195, acc 0.625\n",
      "2018-07-18T15:45:02.154197: step 802, loss 1.00658, acc 0.609375\n",
      "2018-07-18T15:45:14.103237: step 803, loss 0.826147, acc 0.671875\n",
      "2018-07-18T15:45:26.997748: step 804, loss 0.866127, acc 0.703125\n",
      "2018-07-18T15:45:38.976709: step 805, loss 0.934167, acc 0.609375\n",
      "2018-07-18T15:45:51.084325: step 806, loss 0.934488, acc 0.625\n",
      "2018-07-18T15:46:02.574593: step 807, loss 0.709661, acc 0.75\n",
      "2018-07-18T15:46:13.268988: step 808, loss 0.886495, acc 0.640625\n",
      "2018-07-18T15:46:23.416847: step 809, loss 1.09252, acc 0.546875\n",
      "2018-07-18T15:46:33.517831: step 810, loss 1.00926, acc 0.515625\n",
      "2018-07-18T15:46:43.838753: step 811, loss 1.16749, acc 0.609375\n",
      "2018-07-18T15:46:54.439400: step 812, loss 0.866425, acc 0.703125\n",
      "2018-07-18T15:47:05.014118: step 813, loss 0.959327, acc 0.609375\n",
      "2018-07-18T15:47:15.324541: step 814, loss 0.839835, acc 0.71875\n",
      "2018-07-18T15:47:25.254980: step 815, loss 0.887299, acc 0.671875\n",
      "2018-07-18T15:47:36.045120: step 816, loss 1.02617, acc 0.65625\n",
      "2018-07-18T15:47:46.190983: step 817, loss 0.855454, acc 0.625\n",
      "2018-07-18T15:47:56.860446: step 818, loss 0.857574, acc 0.640625\n",
      "2018-07-18T15:48:08.233028: step 819, loss 0.709202, acc 0.703125\n",
      "2018-07-18T15:48:21.163444: step 820, loss 0.718113, acc 0.734375\n",
      "2018-07-18T15:48:33.249120: step 821, loss 0.828567, acc 0.71875\n",
      "2018-07-18T15:48:45.247029: step 822, loss 0.979555, acc 0.546875\n",
      "2018-07-18T15:48:57.265884: step 823, loss 1.16897, acc 0.59375\n",
      "2018-07-18T15:49:07.545388: step 824, loss 1.0771, acc 0.53125\n",
      "2018-07-18T15:49:18.389384: step 825, loss 0.843399, acc 0.625\n",
      "2018-07-18T15:49:28.323814: step 826, loss 1.0026, acc 0.65625\n",
      "2018-07-18T15:49:37.322744: step 827, loss 0.838349, acc 0.71875\n",
      "2018-07-18T15:49:45.994550: step 828, loss 0.885982, acc 0.671875\n",
      "2018-07-18T15:49:54.847870: step 829, loss 0.894657, acc 0.6875\n",
      "2018-07-18T15:50:03.561565: step 830, loss 0.966873, acc 0.578125\n",
      "2018-07-18T15:50:12.117680: step 831, loss 0.943485, acc 0.609375\n",
      "2018-07-18T15:50:20.723671: step 832, loss 0.912105, acc 0.6875\n",
      "2018-07-18T15:50:29.807367: step 833, loss 0.808844, acc 0.6875\n",
      "2018-07-18T15:50:38.452245: step 834, loss 0.79377, acc 0.671875\n",
      "2018-07-18T15:50:47.136018: step 835, loss 0.971615, acc 0.65625\n",
      "2018-07-18T15:50:56.175839: step 836, loss 1.06234, acc 0.609375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-18T15:51:06.452353: step 837, loss 0.755338, acc 0.703125\n",
      "2018-07-18T15:51:14.923695: step 838, loss 0.768487, acc 0.734375\n",
      "2018-07-18T15:51:23.404014: step 839, loss 0.738752, acc 0.75\n",
      "2018-07-18T15:51:33.204800: step 840, loss 0.841594, acc 0.703125\n",
      "2018-07-18T15:51:43.199069: step 841, loss 0.899648, acc 0.625\n",
      "2018-07-18T15:51:53.106570: step 842, loss 0.732261, acc 0.765625\n",
      "2018-07-18T15:52:02.713875: step 843, loss 0.936436, acc 0.640625\n",
      "2018-07-18T15:52:11.574176: step 844, loss 1.20209, acc 0.5625\n",
      "2018-07-18T15:52:20.631949: step 845, loss 1.01702, acc 0.5625\n",
      "2018-07-18T15:52:26.440414: step 846, loss 0.825917, acc 0.7\n",
      "2018-07-18T15:52:36.010815: step 847, loss 1.0417, acc 0.65625\n",
      "2018-07-18T15:52:45.842518: step 848, loss 0.857815, acc 0.6875\n",
      "2018-07-18T15:52:56.823151: step 849, loss 0.891889, acc 0.703125\n",
      "2018-07-18T15:53:07.690085: step 850, loss 0.803552, acc 0.671875\n",
      "2018-07-18T15:53:18.545051: step 851, loss 0.913065, acc 0.6875\n",
      "2018-07-18T15:53:28.269043: step 852, loss 0.820238, acc 0.734375\n",
      "2018-07-18T15:53:37.347761: step 853, loss 0.901717, acc 0.609375\n",
      "2018-07-18T15:53:46.902206: step 854, loss 0.989869, acc 0.59375\n",
      "2018-07-18T15:53:55.821350: step 855, loss 0.810627, acc 0.765625\n",
      "2018-07-18T15:54:06.064953: step 856, loss 1.18121, acc 0.5625\n",
      "2018-07-18T15:54:15.195532: step 857, loss 0.758811, acc 0.6875\n",
      "2018-07-18T15:54:25.400237: step 858, loss 0.599636, acc 0.765625\n",
      "2018-07-18T15:54:35.472298: step 859, loss 0.881237, acc 0.609375\n",
      "2018-07-18T15:54:44.263784: step 860, loss 1.03028, acc 0.578125\n",
      "2018-07-18T15:54:53.370426: step 861, loss 0.817061, acc 0.671875\n",
      "2018-07-18T15:55:02.882984: step 862, loss 0.893977, acc 0.671875\n",
      "2018-07-18T15:55:11.935771: step 863, loss 0.842579, acc 0.6875\n",
      "2018-07-18T15:55:20.652456: step 864, loss 1.0426, acc 0.640625\n",
      "2018-07-18T15:55:30.223856: step 865, loss 0.797739, acc 0.65625\n",
      "2018-07-18T15:55:38.893668: step 866, loss 0.92514, acc 0.625\n",
      "2018-07-18T15:55:47.535553: step 867, loss 0.875923, acc 0.734375\n",
      "2018-07-18T15:55:56.266202: step 868, loss 0.859574, acc 0.703125\n",
      "2018-07-18T15:56:07.753478: step 869, loss 0.920547, acc 0.625\n",
      "2018-07-18T15:56:18.753058: step 870, loss 0.853902, acc 0.671875\n",
      "2018-07-18T15:56:28.907898: step 871, loss 0.766527, acc 0.71875\n",
      "2018-07-18T15:56:40.468975: step 872, loss 1.12081, acc 0.546875\n",
      "2018-07-18T15:56:50.754465: step 873, loss 1.04693, acc 0.53125\n",
      "2018-07-18T15:57:00.749732: step 874, loss 0.932715, acc 0.6875\n",
      "2018-07-18T15:57:13.642249: step 875, loss 0.866037, acc 0.6875\n",
      "2018-07-18T15:57:27.209960: step 876, loss 0.908161, acc 0.703125\n",
      "2018-07-18T15:57:40.172290: step 877, loss 0.678133, acc 0.734375\n",
      "2018-07-18T15:57:51.929843: step 878, loss 0.728873, acc 0.78125\n",
      "2018-07-18T15:58:02.687070: step 879, loss 1.0916, acc 0.53125\n",
      "2018-07-18T15:58:12.874821: step 880, loss 0.721144, acc 0.78125\n",
      "2018-07-18T15:58:23.170285: step 881, loss 0.851711, acc 0.65625\n",
      "2018-07-18T15:58:33.596399: step 882, loss 1.04442, acc 0.625\n",
      "2018-07-18T15:58:44.177099: step 883, loss 0.86617, acc 0.671875\n",
      "2018-07-18T15:58:55.152742: step 884, loss 0.905252, acc 0.71875\n",
      "2018-07-18T15:59:04.439904: step 885, loss 0.820639, acc 0.703125\n",
      "2018-07-18T15:59:13.545550: step 886, loss 0.838265, acc 0.640625\n",
      "2018-07-18T15:59:22.286171: step 887, loss 0.874693, acc 0.703125\n",
      "2018-07-18T15:59:31.154452: step 888, loss 0.924767, acc 0.609375\n",
      "2018-07-18T15:59:39.977853: step 889, loss 0.814907, acc 0.75\n",
      "2018-07-18T15:59:49.835485: step 890, loss 0.968188, acc 0.609375\n",
      "2018-07-18T15:59:59.750965: step 891, loss 1.18662, acc 0.546875\n",
      "2018-07-18T16:00:09.322366: step 892, loss 0.903871, acc 0.6875\n",
      "2018-07-18T16:00:18.400086: step 893, loss 0.824299, acc 0.71875\n",
      "2018-07-18T16:00:28.522013: step 894, loss 0.902529, acc 0.71875\n",
      "2018-07-18T16:00:38.086431: step 895, loss 0.927231, acc 0.65625\n",
      "2018-07-18T16:00:48.270193: step 896, loss 0.819409, acc 0.625\n",
      "2018-07-18T16:01:01.874825: step 897, loss 0.748901, acc 0.6875\n",
      "2018-07-18T16:01:16.380009: step 898, loss 0.879445, acc 0.703125\n",
      "2018-07-18T16:01:29.478974: step 899, loss 0.988222, acc 0.59375\n",
      "2018-07-18T16:01:39.938000: step 900, loss 0.777381, acc 0.671875\n",
      "\n",
      "Evaluation:\n",
      "2018-07-18T16:03:34.651182: step 900, loss 0.839446, acc 0.719\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\dkdk6\\cnn-text-classification-tf-master\\runs\\1531944623\\checkpoints\\model-900\n",
      "\n",
      "2018-07-18T16:03:46.240185: step 901, loss 0.713745, acc 0.765625\n",
      "2018-07-18T16:03:59.071865: step 902, loss 1.00228, acc 0.625\n",
      "2018-07-18T16:04:10.388597: step 903, loss 1.07454, acc 0.640625\n",
      "2018-07-18T16:04:20.999218: step 904, loss 0.89082, acc 0.671875\n",
      "2018-07-18T16:04:30.945614: step 905, loss 0.958702, acc 0.640625\n",
      "2018-07-18T16:04:41.382701: step 906, loss 0.868676, acc 0.65625\n",
      "2018-07-18T16:04:52.004289: step 907, loss 0.902759, acc 0.6875\n",
      "2018-07-18T16:05:02.219967: step 908, loss 0.910192, acc 0.703125\n",
      "2018-07-18T16:05:12.561306: step 909, loss 0.757327, acc 0.734375\n",
      "2018-07-18T16:05:24.131976: step 910, loss 0.895114, acc 0.71875\n",
      "2018-07-18T16:05:35.092859: step 911, loss 0.764773, acc 0.71875\n",
      "2018-07-18T16:05:46.254003: step 912, loss 0.808558, acc 0.6875\n",
      "2018-07-18T16:05:56.843679: step 913, loss 0.900429, acc 0.71875\n",
      "2018-07-18T16:06:07.829296: step 914, loss 0.871726, acc 0.671875\n",
      "2018-07-18T16:06:18.252417: step 915, loss 0.848383, acc 0.625\n",
      "2018-07-18T16:06:28.084122: step 916, loss 0.68449, acc 0.796875\n",
      "2018-07-18T16:06:39.670133: step 917, loss 0.930487, acc 0.625\n",
      "2018-07-18T16:06:50.736534: step 918, loss 1.03808, acc 0.671875\n",
      "2018-07-18T16:07:02.611772: step 919, loss 0.807727, acc 0.65625\n",
      "2018-07-18T16:07:14.655559: step 920, loss 0.944683, acc 0.671875\n",
      "2018-07-18T16:07:26.426077: step 921, loss 0.85867, acc 0.671875\n",
      "2018-07-18T16:07:38.525340: step 922, loss 0.988954, acc 0.578125\n",
      "2018-07-18T16:07:51.392924: step 923, loss 0.79174, acc 0.703125\n",
      "2018-07-18T16:08:03.489571: step 924, loss 0.998717, acc 0.59375\n",
      "2018-07-18T16:08:15.677937: step 925, loss 0.976148, acc 0.5625\n",
      "2018-07-18T16:08:28.878629: step 926, loss 0.880235, acc 0.734375\n",
      "2018-07-18T16:08:41.266006: step 927, loss 0.804903, acc 0.6875\n",
      "2018-07-18T16:08:52.937788: step 928, loss 1.00438, acc 0.6875\n",
      "2018-07-18T16:09:02.698681: step 929, loss 0.845375, acc 0.6875\n",
      "2018-07-18T16:09:12.097543: step 930, loss 0.654994, acc 0.796875\n",
      "2018-07-18T16:09:21.469475: step 931, loss 0.768828, acc 0.75\n",
      "2018-07-18T16:09:30.448460: step 932, loss 0.691308, acc 0.765625\n",
      "2018-07-18T16:09:39.142208: step 933, loss 0.837226, acc 0.6875\n",
      "2018-07-18T16:09:48.021458: step 934, loss 1.10884, acc 0.53125\n",
      "2018-07-18T16:09:59.775021: step 935, loss 0.910234, acc 0.65625\n",
      "2018-07-18T16:10:11.809476: step 936, loss 0.725944, acc 0.734375\n",
      "2018-07-18T16:10:24.617219: step 937, loss 0.966471, acc 0.515625\n",
      "2018-07-18T16:10:37.300296: step 938, loss 0.750947, acc 0.765625\n",
      "2018-07-18T16:10:47.324484: step 939, loss 0.898493, acc 0.671875\n",
      "2018-07-18T16:10:59.154842: step 940, loss 0.690491, acc 0.75\n",
      "2018-07-18T16:11:11.073984: step 941, loss 0.790908, acc 0.703125\n",
      "2018-07-18T16:11:22.666955: step 942, loss 0.780357, acc 0.734375\n",
      "2018-07-18T16:11:34.491328: step 943, loss 1.04122, acc 0.609375\n",
      "2018-07-18T16:11:45.869895: step 944, loss 0.959103, acc 0.59375\n",
      "2018-07-18T16:11:58.990802: step 945, loss 0.733065, acc 0.765625\n",
      "2018-07-18T16:12:10.759325: step 946, loss 0.919379, acc 0.671875\n",
      "2018-07-18T16:12:22.581712: step 947, loss 0.829034, acc 0.71875\n",
      "2018-07-18T16:12:34.836926: step 948, loss 0.782058, acc 0.71875\n",
      "2018-07-18T16:12:48.856429: step 949, loss 0.847126, acc 0.65625\n",
      "2018-07-18T16:13:03.559104: step 950, loss 0.885622, acc 0.59375\n",
      "2018-07-18T16:13:15.740110: step 951, loss 0.769424, acc 0.71875\n",
      "2018-07-18T16:13:27.895599: step 952, loss 0.799185, acc 0.75\n",
      "2018-07-18T16:13:39.256213: step 953, loss 0.74774, acc 0.734375\n",
      "2018-07-18T16:13:51.130452: step 954, loss 0.762954, acc 0.6875\n",
      "2018-07-18T16:14:03.182219: step 955, loss 0.90817, acc 0.640625\n",
      "2018-07-18T16:14:14.905861: step 956, loss 0.90749, acc 0.734375\n",
      "2018-07-18T16:14:26.667403: step 957, loss 1.02278, acc 0.578125\n",
      "2018-07-18T16:14:38.025025: step 958, loss 0.807573, acc 0.734375\n",
      "2018-07-18T16:14:48.926867: step 959, loss 0.866783, acc 0.671875\n",
      "2018-07-18T16:14:59.382901: step 960, loss 0.811761, acc 0.609375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-18T16:15:09.306360: step 961, loss 0.783724, acc 0.671875\n",
      "2018-07-18T16:15:19.304617: step 962, loss 0.854338, acc 0.703125\n",
      "2018-07-18T16:15:29.484390: step 963, loss 0.854832, acc 0.65625\n",
      "2018-07-18T16:15:39.760905: step 964, loss 0.724844, acc 0.703125\n",
      "2018-07-18T16:15:50.172058: step 965, loss 0.885142, acc 0.671875\n",
      "2018-07-18T16:16:00.842518: step 966, loss 0.67828, acc 0.75\n",
      "2018-07-18T16:16:10.451816: step 967, loss 0.778975, acc 0.65625\n",
      "2018-07-18T16:16:20.630593: step 968, loss 1.00321, acc 0.625\n",
      "2018-07-18T16:16:31.307035: step 969, loss 0.825291, acc 0.703125\n",
      "2018-07-18T16:16:43.522609: step 970, loss 0.826767, acc 0.65625\n",
      "2018-07-18T16:16:55.602300: step 971, loss 0.776762, acc 0.75\n",
      "2018-07-18T16:17:04.916388: step 972, loss 0.901103, acc 0.6875\n",
      "2018-07-18T16:17:14.149694: step 973, loss 0.813424, acc 0.625\n",
      "2018-07-18T16:17:24.553865: step 974, loss 0.935324, acc 0.640625\n",
      "2018-07-18T16:17:34.750593: step 975, loss 0.745364, acc 0.71875\n",
      "2018-07-18T16:17:44.710952: step 976, loss 0.725151, acc 0.765625\n",
      "2018-07-18T16:17:54.442923: step 977, loss 0.86761, acc 0.65625\n",
      "2018-07-18T16:18:05.586119: step 978, loss 0.813428, acc 0.734375\n",
      "2018-07-18T16:18:15.462702: step 979, loss 0.843871, acc 0.703125\n",
      "2018-07-18T16:18:24.385835: step 980, loss 0.637899, acc 0.8125\n",
      "2018-07-18T16:18:33.274063: step 981, loss 0.940823, acc 0.59375\n",
      "2018-07-18T16:18:42.010695: step 982, loss 0.934522, acc 0.625\n",
      "2018-07-18T16:18:50.700453: step 983, loss 0.913645, acc 0.609375\n",
      "2018-07-18T16:18:59.452045: step 984, loss 0.94793, acc 0.671875\n",
      "2018-07-18T16:19:08.074982: step 985, loss 1.1029, acc 0.625\n",
      "2018-07-18T16:19:16.737812: step 986, loss 0.824952, acc 0.703125\n",
      "2018-07-18T16:19:21.989765: step 987, loss 0.901636, acc 0.725\n",
      "2018-07-18T16:19:30.787236: step 988, loss 0.836102, acc 0.78125\n",
      "2018-07-18T16:19:39.656511: step 989, loss 0.897685, acc 0.65625\n",
      "2018-07-18T16:19:48.460963: step 990, loss 0.518505, acc 0.78125\n",
      "2018-07-18T16:19:57.387089: step 991, loss 0.896422, acc 0.6875\n",
      "2018-07-18T16:20:06.273322: step 992, loss 0.986108, acc 0.609375\n",
      "2018-07-18T16:20:15.117666: step 993, loss 0.903821, acc 0.640625\n",
      "2018-07-18T16:20:24.146518: step 994, loss 0.831956, acc 0.703125\n",
      "2018-07-18T16:20:33.082616: step 995, loss 0.879167, acc 0.640625\n",
      "2018-07-18T16:20:41.957878: step 996, loss 0.832587, acc 0.703125\n",
      "2018-07-18T16:20:51.071502: step 997, loss 0.710089, acc 0.71875\n",
      "2018-07-18T16:21:00.150221: step 998, loss 0.757755, acc 0.703125\n",
      "2018-07-18T16:21:08.988580: step 999, loss 0.70694, acc 0.71875\n",
      "2018-07-18T16:21:18.062311: step 1000, loss 0.753999, acc 0.6875\n",
      "\n",
      "Evaluation:\n",
      "2018-07-18T16:22:58.061403: step 1000, loss 0.822589, acc 0.706\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\dkdk6\\cnn-text-classification-tf-master\\runs\\1531944623\\checkpoints\\model-1000\n",
      "\n",
      "2018-07-18T16:23:08.040713: step 1001, loss 0.618108, acc 0.78125\n",
      "2018-07-18T16:23:17.301943: step 1002, loss 0.672343, acc 0.8125\n",
      "2018-07-18T16:23:27.299203: step 1003, loss 0.755226, acc 0.71875\n",
      "2018-07-18T16:23:36.091686: step 1004, loss 0.77277, acc 0.734375\n",
      "2018-07-18T16:23:45.014820: step 1005, loss 0.82344, acc 0.78125\n",
      "2018-07-18T16:23:54.708892: step 1006, loss 0.991922, acc 0.640625\n",
      "2018-07-18T16:24:06.178216: step 1007, loss 0.825523, acc 0.75\n",
      "2018-07-18T16:24:18.816702: step 1008, loss 0.863213, acc 0.6875\n",
      "2018-07-18T16:24:32.200904: step 1009, loss 0.672744, acc 0.828125\n",
      "2018-07-18T16:24:44.199812: step 1010, loss 0.736218, acc 0.75\n",
      "2018-07-18T16:24:55.884558: step 1011, loss 0.905211, acc 0.59375\n",
      "2018-07-18T16:25:06.982875: step 1012, loss 0.861055, acc 0.71875\n",
      "2018-07-18T16:25:17.127741: step 1013, loss 0.774136, acc 0.765625\n",
      "2018-07-18T16:25:26.665232: step 1014, loss 0.72719, acc 0.71875\n",
      "2018-07-18T16:25:37.079377: step 1015, loss 0.695129, acc 0.71875\n",
      "2018-07-18T16:25:47.175375: step 1016, loss 0.657067, acc 0.796875\n",
      "2018-07-18T16:25:57.015056: step 1017, loss 0.853433, acc 0.625\n",
      "2018-07-18T16:26:05.830480: step 1018, loss 0.86147, acc 0.65625\n",
      "2018-07-18T16:26:14.524225: step 1019, loss 0.680822, acc 0.78125\n",
      "2018-07-18T16:26:24.069694: step 1020, loss 0.876832, acc 0.65625\n",
      "2018-07-18T16:26:33.899403: step 1021, loss 0.888165, acc 0.671875\n",
      "2018-07-18T16:26:44.417272: step 1022, loss 0.908342, acc 0.6875\n",
      "2018-07-18T16:26:53.793194: step 1023, loss 0.722287, acc 0.765625\n",
      "2018-07-18T16:27:04.542443: step 1024, loss 0.752324, acc 0.71875\n",
      "2018-07-18T16:27:18.108161: step 1025, loss 0.575961, acc 0.78125\n",
      "2018-07-18T16:27:29.223431: step 1026, loss 0.846418, acc 0.71875\n",
      "2018-07-18T16:27:39.118964: step 1027, loss 0.643072, acc 0.78125\n",
      "2018-07-18T16:27:49.051398: step 1028, loss 0.707978, acc 0.6875\n",
      "2018-07-18T16:27:58.267747: step 1029, loss 0.864634, acc 0.765625\n",
      "2018-07-18T16:28:07.652645: step 1030, loss 0.6955, acc 0.734375\n",
      "2018-07-18T16:28:16.615672: step 1031, loss 0.875588, acc 0.703125\n",
      "2018-07-18T16:28:25.512876: step 1032, loss 1.12266, acc 0.625\n",
      "2018-07-18T16:28:34.990527: step 1033, loss 0.804034, acc 0.6875\n",
      "2018-07-18T16:28:45.776677: step 1034, loss 0.876238, acc 0.6875\n",
      "2018-07-18T16:28:55.360045: step 1035, loss 0.950879, acc 0.671875\n",
      "2018-07-18T16:29:04.615290: step 1036, loss 0.876493, acc 0.671875\n",
      "2018-07-18T16:29:13.904445: step 1037, loss 0.734165, acc 0.71875\n",
      "2018-07-18T16:29:23.284357: step 1038, loss 0.687336, acc 0.71875\n",
      "2018-07-18T16:29:33.362401: step 1039, loss 0.859731, acc 0.703125\n",
      "2018-07-18T16:29:43.011594: step 1040, loss 0.847454, acc 0.59375\n",
      "2018-07-18T16:29:53.579329: step 1041, loss 0.581191, acc 0.765625\n",
      "2018-07-18T16:30:03.496802: step 1042, loss 0.93469, acc 0.6875\n",
      "2018-07-18T16:30:12.356107: step 1043, loss 0.761418, acc 0.6875\n",
      "2018-07-18T16:30:21.955433: step 1044, loss 0.814649, acc 0.671875\n",
      "2018-07-18T16:30:31.992586: step 1045, loss 0.763187, acc 0.703125\n",
      "2018-07-18T16:30:40.900759: step 1046, loss 0.783049, acc 0.671875\n",
      "2018-07-18T16:30:50.835188: step 1047, loss 0.900004, acc 0.71875\n",
      "2018-07-18T16:31:00.026605: step 1048, loss 0.83266, acc 0.671875\n",
      "2018-07-18T16:31:09.598004: step 1049, loss 0.871504, acc 0.71875\n",
      "2018-07-18T16:31:19.075656: step 1050, loss 0.755794, acc 0.703125\n",
      "2018-07-18T16:31:28.284027: step 1051, loss 0.858949, acc 0.65625\n",
      "2018-07-18T16:31:37.334818: step 1052, loss 0.582255, acc 0.828125\n",
      "2018-07-18T16:31:46.151238: step 1053, loss 0.682339, acc 0.65625\n",
      "2018-07-18T16:31:54.875905: step 1054, loss 0.855128, acc 0.65625\n",
      "2018-07-18T16:32:04.004487: step 1055, loss 0.863652, acc 0.65625\n",
      "2018-07-18T16:32:12.727158: step 1056, loss 0.854482, acc 0.6875\n",
      "2018-07-18T16:32:23.796550: step 1057, loss 0.841514, acc 0.78125\n",
      "2018-07-18T16:32:33.056783: step 1058, loss 0.557985, acc 0.78125\n",
      "2018-07-18T16:32:41.950993: step 1059, loss 0.726597, acc 0.703125\n",
      "2018-07-18T16:32:51.771725: step 1060, loss 0.832687, acc 0.6875\n",
      "2018-07-18T16:33:01.594453: step 1061, loss 0.706332, acc 0.734375\n",
      "2018-07-18T16:33:10.879620: step 1062, loss 0.670328, acc 0.71875\n",
      "2018-07-18T16:33:19.531477: step 1063, loss 0.865566, acc 0.6875\n",
      "2018-07-18T16:33:28.855538: step 1064, loss 0.726125, acc 0.71875\n",
      "2018-07-18T16:33:37.611121: step 1065, loss 0.836905, acc 0.71875\n",
      "2018-07-18T16:33:46.682859: step 1066, loss 0.679027, acc 0.6875\n",
      "2018-07-18T16:33:55.414503: step 1067, loss 0.854763, acc 0.6875\n",
      "2018-07-18T16:34:04.979433: step 1068, loss 0.61123, acc 0.78125\n",
      "2018-07-18T16:34:13.603400: step 1069, loss 0.741889, acc 0.734375\n",
      "2018-07-18T16:34:22.162509: step 1070, loss 1.00027, acc 0.578125\n",
      "2018-07-18T16:34:30.970948: step 1071, loss 0.96349, acc 0.625\n",
      "2018-07-18T16:34:39.580919: step 1072, loss 0.662473, acc 0.6875\n",
      "2018-07-18T16:34:49.792607: step 1073, loss 1.00887, acc 0.578125\n",
      "2018-07-18T16:34:59.556493: step 1074, loss 0.846038, acc 0.609375\n",
      "2018-07-18T16:35:09.115924: step 1075, loss 0.703297, acc 0.75\n",
      "2018-07-18T16:35:18.082940: step 1076, loss 1.03287, acc 0.625\n",
      "2018-07-18T16:35:27.083866: step 1077, loss 0.729208, acc 0.75\n",
      "2018-07-18T16:35:35.666909: step 1078, loss 0.693673, acc 0.71875\n",
      "2018-07-18T16:35:44.317772: step 1079, loss 0.77344, acc 0.78125\n",
      "2018-07-18T16:35:52.866905: step 1080, loss 0.780811, acc 0.6875\n",
      "2018-07-18T16:36:02.020422: step 1081, loss 0.623538, acc 0.765625\n",
      "2018-07-18T16:36:11.818218: step 1082, loss 0.748688, acc 0.75\n",
      "2018-07-18T16:36:22.427841: step 1083, loss 1.0146, acc 0.609375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-18T16:36:31.811743: step 1084, loss 0.832451, acc 0.703125\n",
      "2018-07-18T16:36:40.451632: step 1085, loss 0.675539, acc 0.75\n",
      "2018-07-18T16:36:50.759063: step 1086, loss 0.680009, acc 0.765625\n",
      "2018-07-18T16:36:59.579472: step 1087, loss 0.726575, acc 0.75\n",
      "2018-07-18T16:37:08.316104: step 1088, loss 0.858564, acc 0.671875\n",
      "2018-07-18T16:37:17.342962: step 1089, loss 0.809587, acc 0.703125\n",
      "2018-07-18T16:37:27.981506: step 1090, loss 0.838878, acc 0.71875\n",
      "2018-07-18T16:37:38.920250: step 1091, loss 0.74223, acc 0.75\n",
      "2018-07-18T16:37:48.235334: step 1092, loss 0.774217, acc 0.734375\n",
      "2018-07-18T16:37:58.345293: step 1093, loss 0.854227, acc 0.734375\n",
      "2018-07-18T16:38:08.991818: step 1094, loss 0.717081, acc 0.703125\n",
      "2018-07-18T16:38:20.331488: step 1095, loss 0.859103, acc 0.65625\n",
      "2018-07-18T16:38:29.564793: step 1096, loss 0.807482, acc 0.734375\n",
      "2018-07-18T16:38:39.216977: step 1097, loss 0.935231, acc 0.65625\n",
      "2018-07-18T16:38:50.257985: step 1098, loss 0.69847, acc 0.765625\n",
      "2018-07-18T16:39:00.971334: step 1099, loss 0.875504, acc 0.6875\n",
      "2018-07-18T16:39:10.700310: step 1100, loss 0.707272, acc 0.71875\n",
      "\n",
      "Evaluation:\n",
      "2018-07-18T16:40:50.511349: step 1100, loss 0.798686, acc 0.71\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\dkdk6\\cnn-text-classification-tf-master\\runs\\1531944623\\checkpoints\\model-1100\n",
      "\n",
      "2018-07-18T16:41:00.847702: step 1101, loss 0.865118, acc 0.703125\n",
      "2018-07-18T16:41:10.894830: step 1102, loss 0.86325, acc 0.5625\n",
      "2018-07-18T16:41:21.898400: step 1103, loss 0.569716, acc 0.828125\n",
      "2018-07-18T16:41:31.570529: step 1104, loss 0.865038, acc 0.71875\n",
      "2018-07-18T16:41:40.760948: step 1105, loss 0.746992, acc 0.765625\n",
      "2018-07-18T16:41:52.588447: step 1106, loss 0.646954, acc 0.8125\n",
      "2018-07-18T16:42:05.280499: step 1107, loss 0.692647, acc 0.8125\n",
      "2018-07-18T16:42:14.319324: step 1108, loss 0.850359, acc 0.703125\n",
      "2018-07-18T16:42:24.605811: step 1109, loss 0.7948, acc 0.765625\n",
      "2018-07-18T16:42:33.138988: step 1110, loss 0.745796, acc 0.6875\n",
      "2018-07-18T16:42:44.014898: step 1111, loss 0.928537, acc 0.609375\n",
      "2018-07-18T16:42:54.331306: step 1112, loss 0.673869, acc 0.734375\n",
      "2018-07-18T16:43:05.436602: step 1113, loss 0.846341, acc 0.65625\n",
      "2018-07-18T16:43:15.929538: step 1114, loss 0.805316, acc 0.671875\n",
      "2018-07-18T16:43:25.436111: step 1115, loss 0.78874, acc 0.6875\n",
      "2018-07-18T16:43:34.667420: step 1116, loss 0.90671, acc 0.671875\n",
      "2018-07-18T16:43:44.832233: step 1117, loss 0.726082, acc 0.671875\n",
      "2018-07-18T16:43:54.535282: step 1118, loss 0.889933, acc 0.65625\n",
      "2018-07-18T16:44:05.098031: step 1119, loss 0.533064, acc 0.8125\n",
      "2018-07-18T16:44:15.524142: step 1120, loss 0.652299, acc 0.765625\n",
      "2018-07-18T16:44:26.792006: step 1121, loss 0.798215, acc 0.703125\n",
      "2018-07-18T16:44:36.054233: step 1122, loss 0.572892, acc 0.796875\n",
      "2018-07-18T16:44:45.126965: step 1123, loss 0.786162, acc 0.6875\n",
      "2018-07-18T16:44:54.051096: step 1124, loss 0.846242, acc 0.59375\n",
      "2018-07-18T16:45:03.028086: step 1125, loss 0.546421, acc 0.828125\n",
      "2018-07-18T16:45:11.830542: step 1126, loss 0.697757, acc 0.71875\n",
      "2018-07-18T16:45:20.802544: step 1127, loss 0.631769, acc 0.78125\n",
      "2018-07-18T16:45:26.290867: step 1128, loss 0.808011, acc 0.675\n",
      "2018-07-18T16:45:35.579024: step 1129, loss 0.744338, acc 0.703125\n",
      "2018-07-18T16:45:44.409406: step 1130, loss 0.691656, acc 0.71875\n",
      "2018-07-18T16:45:53.433270: step 1131, loss 0.769919, acc 0.71875\n",
      "2018-07-18T16:46:02.405273: step 1132, loss 0.656624, acc 0.75\n",
      "2018-07-18T16:46:11.504935: step 1133, loss 0.610345, acc 0.828125\n",
      "2018-07-18T16:46:20.315370: step 1134, loss 0.575487, acc 0.75\n",
      "2018-07-18T16:46:29.156722: step 1135, loss 0.739796, acc 0.71875\n",
      "2018-07-18T16:46:38.022010: step 1136, loss 0.809947, acc 0.71875\n",
      "2018-07-18T16:46:46.718750: step 1137, loss 0.524344, acc 0.78125\n",
      "2018-07-18T16:46:55.498267: step 1138, loss 0.784418, acc 0.75\n",
      "2018-07-18T16:47:05.409758: step 1139, loss 0.623375, acc 0.734375\n",
      "2018-07-18T16:47:14.274049: step 1140, loss 0.701659, acc 0.71875\n",
      "2018-07-18T16:47:22.665605: step 1141, loss 0.754131, acc 0.703125\n",
      "2018-07-18T16:47:31.162878: step 1142, loss 0.56929, acc 0.8125\n",
      "2018-07-18T16:47:40.725301: step 1143, loss 0.793635, acc 0.71875\n",
      "2018-07-18T16:47:50.503148: step 1144, loss 0.788344, acc 0.671875\n",
      "2018-07-18T16:47:59.832197: step 1145, loss 0.554195, acc 0.78125\n",
      "2018-07-18T16:48:09.652930: step 1146, loss 0.844918, acc 0.703125\n",
      "2018-07-18T16:48:18.082385: step 1147, loss 0.750269, acc 0.71875\n",
      "2018-07-18T16:48:26.554724: step 1148, loss 0.514314, acc 0.78125\n",
      "2018-07-18T16:48:35.104855: step 1149, loss 0.888187, acc 0.65625\n",
      "2018-07-18T16:48:43.738762: step 1150, loss 0.49126, acc 0.890625\n",
      "2018-07-18T16:48:52.670873: step 1151, loss 0.647703, acc 0.75\n",
      "2018-07-18T16:49:01.208036: step 1152, loss 0.637292, acc 0.796875\n",
      "2018-07-18T16:49:09.904777: step 1153, loss 0.668254, acc 0.78125\n",
      "2018-07-18T16:49:20.160346: step 1154, loss 0.812626, acc 0.71875\n",
      "2018-07-18T16:49:29.999031: step 1155, loss 0.722178, acc 0.796875\n",
      "2018-07-18T16:49:39.228348: step 1156, loss 0.611791, acc 0.78125\n",
      "2018-07-18T16:49:49.177735: step 1157, loss 0.776012, acc 0.703125\n",
      "2018-07-18T16:49:59.521071: step 1158, loss 0.722938, acc 0.734375\n",
      "2018-07-18T16:50:09.042604: step 1159, loss 0.609025, acc 0.78125\n",
      "2018-07-18T16:50:19.586402: step 1160, loss 0.625535, acc 0.8125\n",
      "2018-07-18T16:50:29.334329: step 1161, loss 0.502791, acc 0.796875\n",
      "2018-07-18T16:50:38.673352: step 1162, loss 0.73998, acc 0.765625\n",
      "2018-07-18T16:50:47.891695: step 1163, loss 0.708436, acc 0.78125\n",
      "2018-07-18T16:50:58.241016: step 1164, loss 0.738915, acc 0.703125\n",
      "2018-07-18T16:51:08.809747: step 1165, loss 0.601135, acc 0.78125\n",
      "2018-07-18T16:51:18.865851: step 1166, loss 0.703189, acc 0.75\n",
      "2018-07-18T16:51:27.983467: step 1167, loss 0.687292, acc 0.75\n",
      "2018-07-18T16:51:36.989377: step 1168, loss 0.698936, acc 0.765625\n",
      "2018-07-18T16:51:46.209715: step 1169, loss 0.635253, acc 0.828125\n",
      "2018-07-18T16:51:55.173741: step 1170, loss 0.751679, acc 0.75\n",
      "2018-07-18T16:52:04.540687: step 1171, loss 0.894861, acc 0.71875\n",
      "2018-07-18T16:52:13.414951: step 1172, loss 0.600337, acc 0.78125\n",
      "2018-07-18T16:52:22.334096: step 1173, loss 0.5761, acc 0.8125\n",
      "2018-07-18T16:52:31.293132: step 1174, loss 0.711058, acc 0.78125\n",
      "2018-07-18T16:52:40.088607: step 1175, loss 0.6377, acc 0.765625\n",
      "2018-07-18T16:52:48.910012: step 1176, loss 0.604809, acc 0.71875\n",
      "2018-07-18T16:52:58.534272: step 1177, loss 0.553252, acc 0.78125\n",
      "2018-07-18T16:53:09.235650: step 1178, loss 0.729178, acc 0.71875\n",
      "2018-07-18T16:53:18.665428: step 1179, loss 0.773933, acc 0.703125\n",
      "2018-07-18T16:53:30.366133: step 1180, loss 0.712659, acc 0.75\n",
      "2018-07-18T16:53:40.623697: step 1181, loss 0.481456, acc 0.921875\n",
      "2018-07-18T16:53:50.066443: step 1182, loss 0.52885, acc 0.78125\n",
      "2018-07-18T16:53:58.942700: step 1183, loss 0.760189, acc 0.671875\n",
      "2018-07-18T16:54:08.167030: step 1184, loss 0.577408, acc 0.8125\n",
      "2018-07-18T16:54:17.090162: step 1185, loss 0.839887, acc 0.703125\n",
      "2018-07-18T16:54:25.901597: step 1186, loss 0.734711, acc 0.78125\n",
      "2018-07-18T16:54:34.752920: step 1187, loss 0.83514, acc 0.734375\n",
      "2018-07-18T16:54:43.513490: step 1188, loss 0.759718, acc 0.71875\n",
      "2018-07-18T16:54:52.397727: step 1189, loss 0.687574, acc 0.703125\n",
      "2018-07-18T16:55:01.224120: step 1190, loss 0.711335, acc 0.734375\n",
      "2018-07-18T16:55:10.013611: step 1191, loss 0.833898, acc 0.6875\n",
      "2018-07-18T16:55:18.809086: step 1192, loss 0.634316, acc 0.734375\n",
      "2018-07-18T16:55:27.889798: step 1193, loss 0.715728, acc 0.765625\n",
      "2018-07-18T16:55:37.569908: step 1194, loss 0.563056, acc 0.796875\n",
      "2018-07-18T16:55:46.518972: step 1195, loss 0.640133, acc 0.828125\n",
      "2018-07-18T16:55:55.617636: step 1196, loss 0.47877, acc 0.921875\n",
      "2018-07-18T16:56:04.561714: step 1197, loss 0.716591, acc 0.6875\n",
      "2018-07-18T16:56:13.633451: step 1198, loss 0.684366, acc 0.703125\n",
      "2018-07-18T16:56:23.631709: step 1199, loss 0.67327, acc 0.71875\n",
      "2018-07-18T16:56:33.252974: step 1200, loss 0.760928, acc 0.75\n",
      "\n",
      "Evaluation:\n",
      "2018-07-18T16:58:04.342829: step 1200, loss 0.762857, acc 0.728\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\dkdk6\\cnn-text-classification-tf-master\\runs\\1531944623\\checkpoints\\model-1200\n",
      "\n",
      "2018-07-18T16:58:16.581096: step 1201, loss 0.556131, acc 0.84375\n",
      "2018-07-18T16:58:25.436411: step 1202, loss 0.54763, acc 0.78125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-18T16:58:33.933685: step 1203, loss 0.685553, acc 0.765625\n",
      "2018-07-18T16:58:42.357156: step 1204, loss 0.78852, acc 0.703125\n",
      "2018-07-18T16:58:50.734747: step 1205, loss 0.908875, acc 0.6875\n",
      "2018-07-18T16:58:59.330757: step 1206, loss 0.859608, acc 0.71875\n",
      "2018-07-18T16:59:07.981619: step 1207, loss 0.884614, acc 0.671875\n",
      "2018-07-18T16:59:16.659408: step 1208, loss 0.810946, acc 0.71875\n",
      "2018-07-18T16:59:25.332211: step 1209, loss 0.671221, acc 0.78125\n",
      "2018-07-18T16:59:34.411927: step 1210, loss 0.7518, acc 0.71875\n",
      "2018-07-18T16:59:43.173492: step 1211, loss 0.707246, acc 0.75\n",
      "2018-07-18T16:59:51.909128: step 1212, loss 0.52081, acc 0.8125\n",
      "2018-07-18T17:00:01.094560: step 1213, loss 0.783322, acc 0.65625\n",
      "2018-07-18T17:00:10.089501: step 1214, loss 0.739088, acc 0.71875\n",
      "2018-07-18T17:00:20.241348: step 1215, loss 0.611592, acc 0.78125\n",
      "2018-07-18T17:00:29.965340: step 1216, loss 0.48556, acc 0.859375\n",
      "2018-07-18T17:00:39.788068: step 1217, loss 0.708775, acc 0.75\n",
      "2018-07-18T17:00:50.710853: step 1218, loss 0.605238, acc 0.8125\n",
      "2018-07-18T17:01:01.758306: step 1219, loss 0.802949, acc 0.703125\n",
      "2018-07-18T17:01:13.692386: step 1220, loss 0.6376, acc 0.734375\n",
      "2018-07-18T17:01:24.312979: step 1221, loss 0.700509, acc 0.828125\n",
      "2018-07-18T17:01:34.454853: step 1222, loss 0.841511, acc 0.703125\n",
      "2018-07-18T17:01:45.705762: step 1223, loss 0.640816, acc 0.75\n",
      "2018-07-18T17:01:55.770841: step 1224, loss 0.653492, acc 0.75\n",
      "2018-07-18T17:02:05.846892: step 1225, loss 0.63561, acc 0.71875\n",
      "2018-07-18T17:02:16.698085: step 1226, loss 0.800746, acc 0.75\n",
      "2018-07-18T17:02:27.093281: step 1227, loss 0.626238, acc 0.75\n",
      "2018-07-18T17:02:36.786356: step 1228, loss 0.856768, acc 0.71875\n",
      "2018-07-18T17:02:47.273307: step 1229, loss 0.808025, acc 0.71875\n",
      "2018-07-18T17:02:57.427149: step 1230, loss 0.647535, acc 0.78125\n",
      "2018-07-18T17:03:07.085316: step 1231, loss 0.524869, acc 0.828125\n",
      "2018-07-18T17:03:17.263094: step 1232, loss 0.785242, acc 0.703125\n",
      "2018-07-18T17:03:28.579827: step 1233, loss 0.828004, acc 0.703125\n",
      "2018-07-18T17:03:39.057801: step 1234, loss 0.671272, acc 0.828125\n",
      "2018-07-18T17:03:47.917106: step 1235, loss 0.772283, acc 0.765625\n",
      "2018-07-18T17:03:56.953935: step 1236, loss 0.64538, acc 0.765625\n",
      "2018-07-18T17:04:05.975806: step 1237, loss 0.660601, acc 0.71875\n",
      "2018-07-18T17:04:14.729393: step 1238, loss 0.798954, acc 0.671875\n",
      "2018-07-18T17:04:23.582712: step 1239, loss 0.796673, acc 0.671875\n",
      "2018-07-18T17:04:32.353254: step 1240, loss 0.782914, acc 0.75\n",
      "2018-07-18T17:04:41.805971: step 1241, loss 0.573175, acc 0.875\n",
      "2018-07-18T17:04:52.591124: step 1242, loss 0.775164, acc 0.703125\n",
      "2018-07-18T17:05:02.924487: step 1243, loss 0.717274, acc 0.796875\n",
      "2018-07-18T17:05:12.675406: step 1244, loss 0.589365, acc 0.78125\n",
      "2018-07-18T17:05:23.644069: step 1245, loss 0.591315, acc 0.75\n",
      "2018-07-18T17:05:33.786940: step 1246, loss 0.771838, acc 0.6875\n",
      "2018-07-18T17:05:44.443438: step 1247, loss 0.701742, acc 0.734375\n",
      "2018-07-18T17:05:57.432708: step 1248, loss 0.780014, acc 0.6875\n",
      "2018-07-18T17:06:08.621817: step 1249, loss 0.583162, acc 0.8125\n",
      "2018-07-18T17:06:19.119737: step 1250, loss 0.585111, acc 0.796875\n",
      "2018-07-18T17:06:31.968373: step 1251, loss 0.736258, acc 0.765625\n",
      "2018-07-18T17:06:43.083643: step 1252, loss 0.992956, acc 0.640625\n",
      "2018-07-18T17:06:54.704560: step 1253, loss 0.722824, acc 0.71875\n",
      "2018-07-18T17:07:05.486721: step 1254, loss 0.788231, acc 0.6875\n",
      "2018-07-18T17:07:16.806446: step 1255, loss 0.663574, acc 0.765625\n",
      "2018-07-18T17:07:27.837941: step 1256, loss 0.583855, acc 0.78125\n",
      "2018-07-18T17:07:39.438913: step 1257, loss 0.88582, acc 0.65625\n",
      "2018-07-18T17:07:50.745669: step 1258, loss 0.743403, acc 0.671875\n",
      "2018-07-18T17:08:02.345645: step 1259, loss 0.55368, acc 0.796875\n",
      "2018-07-18T17:08:13.238509: step 1260, loss 0.734971, acc 0.703125\n",
      "2018-07-18T17:08:24.008702: step 1261, loss 0.741017, acc 0.671875\n",
      "2018-07-18T17:08:36.274895: step 1262, loss 0.724891, acc 0.71875\n",
      "2018-07-18T17:08:47.257520: step 1263, loss 0.586403, acc 0.75\n",
      "2018-07-18T17:08:58.167340: step 1264, loss 0.560145, acc 0.8125\n",
      "2018-07-18T17:09:09.008344: step 1265, loss 0.703394, acc 0.734375\n",
      "2018-07-18T17:09:19.494299: step 1266, loss 0.578372, acc 0.796875\n",
      "2018-07-18T17:09:30.437032: step 1267, loss 0.786012, acc 0.78125\n",
      "2018-07-18T17:09:41.579228: step 1268, loss 0.9173, acc 0.640625\n",
      "2018-07-18T17:09:48.253378: step 1269, loss 0.722636, acc 0.8\n",
      "2018-07-18T17:09:58.975699: step 1270, loss 0.71259, acc 0.65625\n",
      "2018-07-18T17:10:09.771837: step 1271, loss 0.687651, acc 0.734375\n",
      "2018-07-18T17:10:20.871136: step 1272, loss 0.534912, acc 0.84375\n",
      "2018-07-18T17:10:32.113069: step 1273, loss 0.607197, acc 0.875\n",
      "2018-07-18T17:10:41.766248: step 1274, loss 0.466138, acc 0.859375\n",
      "2018-07-18T17:10:54.757503: step 1275, loss 0.519377, acc 0.78125\n",
      "2018-07-18T17:11:07.753742: step 1276, loss 0.623588, acc 0.796875\n",
      "2018-07-18T17:11:20.809820: step 1277, loss 0.42565, acc 0.90625\n",
      "2018-07-18T17:11:32.995229: step 1278, loss 0.472785, acc 0.828125\n",
      "2018-07-18T17:11:45.067951: step 1279, loss 0.59206, acc 0.78125\n",
      "2018-07-18T17:11:56.410601: step 1280, loss 0.568447, acc 0.78125\n",
      "2018-07-18T17:12:06.640243: step 1281, loss 0.547639, acc 0.84375\n",
      "2018-07-18T17:12:17.809375: step 1282, loss 0.627059, acc 0.78125\n",
      "2018-07-18T17:12:29.202902: step 1283, loss 0.630448, acc 0.78125\n",
      "2018-07-18T17:12:40.487710: step 1284, loss 0.484089, acc 0.859375\n",
      "2018-07-18T17:12:50.242620: step 1285, loss 0.526411, acc 0.78125\n",
      "2018-07-18T17:13:00.588947: step 1286, loss 0.64278, acc 0.765625\n",
      "2018-07-18T17:13:10.669983: step 1287, loss 0.642944, acc 0.734375\n",
      "2018-07-18T17:13:21.485057: step 1288, loss 0.639063, acc 0.78125\n",
      "2018-07-18T17:13:32.244278: step 1289, loss 0.80871, acc 0.6875\n",
      "2018-07-18T17:13:43.588935: step 1290, loss 0.686254, acc 0.71875\n",
      "2018-07-18T17:13:53.565254: step 1291, loss 0.604331, acc 0.703125\n",
      "2018-07-18T17:14:03.881660: step 1292, loss 0.516583, acc 0.8125\n",
      "2018-07-18T17:14:14.355646: step 1293, loss 0.762038, acc 0.75\n",
      "2018-07-18T17:14:24.424714: step 1294, loss 0.438795, acc 0.859375\n",
      "2018-07-18T17:14:35.077223: step 1295, loss 0.5302, acc 0.796875\n",
      "2018-07-18T17:14:45.375678: step 1296, loss 0.676328, acc 0.6875\n",
      "2018-07-18T17:14:55.951392: step 1297, loss 0.611078, acc 0.796875\n",
      "2018-07-18T17:15:06.198984: step 1298, loss 0.581823, acc 0.765625\n",
      "2018-07-18T17:15:16.423636: step 1299, loss 0.763864, acc 0.671875\n",
      "2018-07-18T17:15:26.436855: step 1300, loss 0.610321, acc 0.78125\n",
      "\n",
      "Evaluation:\n",
      "2018-07-18T17:17:15.431330: step 1300, loss 0.724776, acc 0.746\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\dkdk6\\cnn-text-classification-tf-master\\runs\\1531944623\\checkpoints\\model-1300\n",
      "\n",
      "2018-07-18T17:17:25.006720: step 1301, loss 0.509451, acc 0.796875\n",
      "2018-07-18T17:17:35.004978: step 1302, loss 0.450848, acc 0.859375\n",
      "2018-07-18T17:17:45.221652: step 1303, loss 0.409538, acc 0.84375\n",
      "2018-07-18T17:17:55.234870: step 1304, loss 0.630238, acc 0.765625\n",
      "2018-07-18T17:18:04.858132: step 1305, loss 0.438051, acc 0.859375\n",
      "2018-07-18T17:18:14.240039: step 1306, loss 0.646604, acc 0.8125\n",
      "2018-07-18T17:18:25.934760: step 1307, loss 0.511985, acc 0.828125\n",
      "2018-07-18T17:18:36.140463: step 1308, loss 0.589453, acc 0.78125\n",
      "2018-07-18T17:18:46.119771: step 1309, loss 0.609755, acc 0.796875\n",
      "2018-07-18T17:18:57.316822: step 1310, loss 0.536453, acc 0.859375\n",
      "2018-07-18T17:19:07.869598: step 1311, loss 0.604384, acc 0.796875\n",
      "2018-07-18T17:19:17.826965: step 1312, loss 0.545097, acc 0.765625\n",
      "2018-07-18T17:19:27.753417: step 1313, loss 0.645929, acc 0.8125\n",
      "2018-07-18T17:19:37.752671: step 1314, loss 0.59002, acc 0.734375\n",
      "2018-07-18T17:19:47.596342: step 1315, loss 0.760703, acc 0.734375\n",
      "2018-07-18T17:19:57.629508: step 1316, loss 0.595955, acc 0.78125\n",
      "2018-07-18T17:20:08.977158: step 1317, loss 0.590828, acc 0.78125\n",
      "2018-07-18T17:20:20.818486: step 1318, loss 0.590286, acc 0.78125\n",
      "2018-07-18T17:20:31.911815: step 1319, loss 0.515062, acc 0.796875\n",
      "2018-07-18T17:20:43.443970: step 1320, loss 0.820191, acc 0.71875\n",
      "2018-07-18T17:20:53.901998: step 1321, loss 0.5649, acc 0.765625\n",
      "2018-07-18T17:21:03.909233: step 1322, loss 0.523385, acc 0.8125\n",
      "2018-07-18T17:21:15.678753: step 1323, loss 0.553794, acc 0.8125\n",
      "2018-07-18T17:21:26.407059: step 1324, loss 0.586786, acc 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-18T17:21:36.245744: step 1325, loss 0.607141, acc 0.828125\n",
      "2018-07-18T17:21:46.090413: step 1326, loss 0.692707, acc 0.71875\n",
      "2018-07-18T17:21:55.663807: step 1327, loss 0.671875, acc 0.71875\n",
      "2018-07-18T17:22:05.668049: step 1328, loss 0.657797, acc 0.796875\n",
      "2018-07-18T17:22:16.090173: step 1329, loss 0.521028, acc 0.765625\n",
      "2018-07-18T17:22:26.316820: step 1330, loss 0.606835, acc 0.78125\n",
      "2018-07-18T17:22:37.234620: step 1331, loss 0.578954, acc 0.78125\n",
      "2018-07-18T17:22:48.427683: step 1332, loss 0.615783, acc 0.796875\n",
      "2018-07-18T17:22:58.704196: step 1333, loss 0.618648, acc 0.765625\n",
      "2018-07-18T17:23:09.306838: step 1334, loss 0.528538, acc 0.796875\n",
      "2018-07-18T17:23:19.050775: step 1335, loss 0.601757, acc 0.796875\n",
      "2018-07-18T17:23:28.326967: step 1336, loss 0.420476, acc 0.84375\n",
      "2018-07-18T17:23:38.914648: step 1337, loss 0.523995, acc 0.875\n",
      "2018-07-18T17:23:49.679854: step 1338, loss 0.405327, acc 0.84375\n",
      "2018-07-18T17:24:03.239586: step 1339, loss 0.526424, acc 0.84375\n",
      "2018-07-18T17:24:14.346886: step 1340, loss 0.523911, acc 0.8125\n",
      "2018-07-18T17:24:26.054565: step 1341, loss 0.608323, acc 0.828125\n",
      "2018-07-18T17:24:36.649227: step 1342, loss 0.51633, acc 0.859375\n",
      "2018-07-18T17:24:47.112242: step 1343, loss 0.467952, acc 0.859375\n",
      "2018-07-18T17:24:57.837556: step 1344, loss 0.625941, acc 0.765625\n",
      "2018-07-18T17:25:09.249035: step 1345, loss 0.610066, acc 0.75\n",
      "2018-07-18T17:25:20.580726: step 1346, loss 0.616647, acc 0.765625\n",
      "2018-07-18T17:25:31.866540: step 1347, loss 0.571349, acc 0.75\n",
      "2018-07-18T17:25:42.316590: step 1348, loss 0.600022, acc 0.734375\n",
      "2018-07-18T17:25:52.333798: step 1349, loss 0.698313, acc 0.75\n",
      "2018-07-18T17:26:02.596349: step 1350, loss 0.659894, acc 0.671875\n",
      "2018-07-18T17:26:12.796068: step 1351, loss 0.647502, acc 0.75\n",
      "2018-07-18T17:26:23.548310: step 1352, loss 0.611152, acc 0.828125\n",
      "2018-07-18T17:26:34.673553: step 1353, loss 0.655855, acc 0.703125\n",
      "2018-07-18T17:26:44.938098: step 1354, loss 0.648158, acc 0.765625\n",
      "2018-07-18T17:26:55.633492: step 1355, loss 0.609525, acc 0.734375\n",
      "2018-07-18T17:27:06.072572: step 1356, loss 0.425057, acc 0.84375\n",
      "2018-07-18T17:27:19.222401: step 1357, loss 0.802967, acc 0.765625\n",
      "2018-07-18T17:27:32.967815: step 1358, loss 0.694992, acc 0.71875\n",
      "2018-07-18T17:27:44.495980: step 1359, loss 0.50957, acc 0.828125\n",
      "2018-07-18T17:27:56.784116: step 1360, loss 0.607277, acc 0.734375\n",
      "2018-07-18T17:28:08.705231: step 1361, loss 0.581157, acc 0.84375\n",
      "2018-07-18T17:28:19.415582: step 1362, loss 0.550109, acc 0.78125\n",
      "2018-07-18T17:28:29.540502: step 1363, loss 0.581614, acc 0.828125\n",
      "2018-07-18T17:28:38.871544: step 1364, loss 0.55285, acc 0.828125\n",
      "2018-07-18T17:28:49.248790: step 1365, loss 0.568938, acc 0.734375\n",
      "2018-07-18T17:28:59.038604: step 1366, loss 0.725755, acc 0.765625\n",
      "2018-07-18T17:29:09.750952: step 1367, loss 0.609771, acc 0.78125\n",
      "2018-07-18T17:29:20.887167: step 1368, loss 0.765551, acc 0.734375\n",
      "2018-07-18T17:29:30.692941: step 1369, loss 0.48412, acc 0.828125\n",
      "2018-07-18T17:29:40.772979: step 1370, loss 0.53091, acc 0.84375\n",
      "2018-07-18T17:29:51.205077: step 1371, loss 0.615804, acc 0.765625\n",
      "2018-07-18T17:30:01.069694: step 1372, loss 0.864521, acc 0.71875\n",
      "2018-07-18T17:30:11.298335: step 1373, loss 0.688998, acc 0.71875\n",
      "2018-07-18T17:30:21.448188: step 1374, loss 0.614442, acc 0.78125\n",
      "2018-07-18T17:30:32.355015: step 1375, loss 0.708433, acc 0.65625\n",
      "2018-07-18T17:30:41.448693: step 1376, loss 0.620875, acc 0.796875\n",
      "2018-07-18T17:30:53.547334: step 1377, loss 0.558597, acc 0.765625\n",
      "2018-07-18T17:31:05.516320: step 1378, loss 0.705947, acc 0.703125\n",
      "2018-07-18T17:31:16.364307: step 1379, loss 0.930155, acc 0.65625\n",
      "2018-07-18T17:31:26.867215: step 1380, loss 0.696803, acc 0.6875\n",
      "2018-07-18T17:31:37.345191: step 1381, loss 0.822051, acc 0.75\n",
      "2018-07-18T17:31:47.282612: step 1382, loss 0.600938, acc 0.734375\n",
      "2018-07-18T17:31:57.277877: step 1383, loss 0.534971, acc 0.84375\n",
      "2018-07-18T17:32:07.134513: step 1384, loss 0.46676, acc 0.8125\n",
      "2018-07-18T17:32:18.511085: step 1385, loss 0.541714, acc 0.828125\n",
      "2018-07-18T17:32:29.091785: step 1386, loss 0.543722, acc 0.84375\n",
      "2018-07-18T17:32:39.241638: step 1387, loss 0.478888, acc 0.859375\n",
      "2018-07-18T17:32:49.138168: step 1388, loss 0.4876, acc 0.828125\n",
      "2018-07-18T17:32:58.995803: step 1389, loss 0.746494, acc 0.6875\n",
      "2018-07-18T17:33:09.169592: step 1390, loss 0.760615, acc 0.765625\n",
      "2018-07-18T17:33:18.951429: step 1391, loss 0.492211, acc 0.828125\n",
      "2018-07-18T17:33:28.439051: step 1392, loss 0.585188, acc 0.78125\n",
      "2018-07-18T17:33:38.282724: step 1393, loss 0.707442, acc 0.84375\n",
      "2018-07-18T17:33:48.989089: step 1394, loss 0.641491, acc 0.71875\n",
      "2018-07-18T17:34:00.123307: step 1395, loss 0.700712, acc 0.734375\n",
      "2018-07-18T17:34:10.580339: step 1396, loss 0.607222, acc 0.75\n",
      "2018-07-18T17:34:21.313631: step 1397, loss 0.450861, acc 0.890625\n",
      "2018-07-18T17:34:31.117409: step 1398, loss 0.547739, acc 0.859375\n",
      "2018-07-18T17:34:39.911887: step 1399, loss 0.613179, acc 0.796875\n",
      "2018-07-18T17:34:49.589021: step 1400, loss 0.596372, acc 0.765625\n",
      "\n",
      "Evaluation:\n",
      "2018-07-18T17:36:30.655685: step 1400, loss 0.71327, acc 0.756\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\dkdk6\\cnn-text-classification-tf-master\\runs\\1531944623\\checkpoints\\model-1400\n",
      "\n",
      "2018-07-18T17:36:41.269298: step 1401, loss 0.641769, acc 0.765625\n",
      "2018-07-18T17:36:51.483978: step 1402, loss 0.534838, acc 0.859375\n",
      "2018-07-18T17:37:01.508166: step 1403, loss 0.684346, acc 0.734375\n",
      "2018-07-18T17:37:11.538339: step 1404, loss 0.4795, acc 0.890625\n",
      "2018-07-18T17:37:22.681541: step 1405, loss 0.817572, acc 0.734375\n",
      "2018-07-18T17:37:31.398221: step 1406, loss 0.708804, acc 0.78125\n",
      "2018-07-18T17:37:40.628532: step 1407, loss 0.742044, acc 0.703125\n",
      "2018-07-18T17:37:50.453255: step 1408, loss 0.716672, acc 0.671875\n",
      "2018-07-18T17:38:00.552243: step 1409, loss 0.64045, acc 0.78125\n",
      "2018-07-18T17:38:07.745005: step 1410, loss 0.687307, acc 0.7\n",
      "OVER\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#! /usr/bin/env python\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import data_helpers\n",
    "from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Data loading params change (2->5)\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "#tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "#tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n",
    "tf.flags.DEFINE_string(\"class1\", \"./data/rt-polaritydata/review1.txt\", \"Data source for the class1 data.\")\n",
    "tf.flags.DEFINE_string(\"class2\", \"./data/rt-polaritydata/review2.txt\", \"Data source for the class2 data.\")\n",
    "tf.flags.DEFINE_string(\"class3\", \"./data/rt-polaritydata/review3.txt\", \"Data source for the class3 data.\")\n",
    "tf.flags.DEFINE_string(\"class4\", \"./data/rt-polaritydata/review4.txt\", \"Data source for the class4 data.\")\n",
    "tf.flags.DEFINE_string(\"class5\", \"./data/rt-polaritydata/review5.txt\", \"Data source for the class5 data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters \n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 10, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "FLAGS = tf.flags.FLAGS\n",
    "# FLAGS._parse_flags()\n",
    "# print(\"\\nParameters:\")\n",
    "# for attr, value in sorted(FLAGS.__flags.items()):\n",
    "#     print(\"{}={}\".format(attr.upper(), value))\n",
    "# print(\"\")\n",
    "\n",
    "def preprocess():\n",
    "    # Data Preparation\n",
    "    # ==================================================\n",
    "\n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    #x_text, y = data_helpers.load_data_and_labels(FLAGS.positive_data_file, FLAGS.negative_data_file) #Maybe..more Flags..\n",
    "    x_text, y = data_helpers.load_data_and_labels(FLAGS.class1, FLAGS.class2,FLAGS.class3,FLAGS.class4,FLAGS.class5) \n",
    "    # Build vocabulary\n",
    "    max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "    x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "    # Randomly shuffle data\n",
    "    np.random.seed(10)\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "    x_shuffled = x[shuffle_indices]\n",
    "    y_shuffled = y[shuffle_indices]\n",
    "\n",
    "    # Split train/test set\n",
    "    # TODO: This is very crude, should use cross-validation\n",
    "    dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
    "    x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "    y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "\n",
    "    del x, y, x_shuffled, y_shuffled\n",
    "\n",
    "    print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "    print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "    return x_train, y_train, vocab_processor, x_dev, y_dev\n",
    "\n",
    "def train(x_train, y_train, vocab_processor, x_dev, y_dev):\n",
    "    # Training\n",
    "    # ==================================================\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "          allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "          log_device_placement=FLAGS.log_device_placement)\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            cnn = TextCNN(\n",
    "                sequence_length=x_train.shape[1],\n",
    "                num_classes=y_train.shape[1], \n",
    "                vocab_size=len(vocab_processor.vocabulary_),\n",
    "                embedding_size=FLAGS.embedding_dim,\n",
    "                filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "                num_filters=FLAGS.num_filters,\n",
    "                l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "            # Define Training procedure\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            # Keep track of gradient values and sparsity (optional)\n",
    "            grad_summaries = []\n",
    "            for g, v in grads_and_vars:\n",
    "                if g is not None:\n",
    "                    grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                    sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                    grad_summaries.append(grad_hist_summary)\n",
    "                    grad_summaries.append(sparsity_summary)\n",
    "            grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "            # Output directory for models and summaries\n",
    "            timestamp = str(int(time.time()))\n",
    "            out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "            print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "            # Summaries for loss and accuracy\n",
    "            loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "            acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "            # Train Summaries\n",
    "            train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "            train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "            # Dev summaries\n",
    "            dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "            dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "            dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "            # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "            # Write vocabulary\n",
    "            vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "            # Initialize all variables\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            def train_step(x_batch, y_batch):\n",
    "                \"\"\"\n",
    "                A single training step\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y: y_batch,\n",
    "                  cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "                }\n",
    "                _, step, summaries, loss, accuracy = sess.run(\n",
    "                    [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "            def dev_step(x_batch, y_batch, writer=None):\n",
    "                \"\"\"\n",
    "                Evaluates model on a dev set\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y: y_batch,\n",
    "                  cnn.dropout_keep_prob: 1.0\n",
    "                }\n",
    "                step, summaries, loss, accuracy = sess.run(\n",
    "                    [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                if writer:\n",
    "                    writer.add_summary(summaries, step)\n",
    "\n",
    "            # Generate batches\n",
    "            batches = data_helpers.batch_iter(\n",
    "                list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "            # Training loop. For each batch...\n",
    "            for batch in batches:\n",
    "                x_batch, y_batch = zip(*batch)\n",
    "                train_step(x_batch, y_batch)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "                if current_step % FLAGS.evaluate_every == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                    print(\"\")\n",
    "                if current_step % FLAGS.checkpoint_every == 0:\n",
    "                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "\n",
    "def main(argv=None):\n",
    "    x_train, y_train, vocab_processor, x_dev, y_dev = preprocess()\n",
    "    train(x_train, y_train, vocab_processor, x_dev, y_dev)\n",
    "    print(\"OVER\")\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start the test\n"
     ]
    }
   ],
   "source": [
    "print(\"Start the test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
